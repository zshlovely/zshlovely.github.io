
<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
  
  
    <meta name="keywords" content="Spark," />
  

  
    <meta name="description" content="既然选择远方，便只顾风雨兼程" />
  
  
  <link rel="icon" type="image/x-icon" href="/logo.png">
  <title>SparkSQL [ shaohua&#39;s blog ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  <div class="nav-container">
    <nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    <img class="avatar" src="/images/logo.png">
    <span class="title">shaohua&#39;s blog</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">归档</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/tags" class="pure-menu-link">标签</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/categories" class="pure-menu-link">分类</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
          
      
  </ul>
   
</nav>
  </div>

  <div class="container" id="content-outer">
    <div class="inner" id="content-inner">
      <div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        SparkSQL
      </h1>
      <span>
        
        <time class="time" datetime="2017-12-31T17:10:20.000Z">
        2018-01-01
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">
      <span id="busuanzi_value_page_pv"></span> 点击
    </span>
      <span class="slash">/</span>
      <span class="read">阅读耗时 10 分钟</span>
    </header>

    <div class="post-content">
      <h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>Spark SQL是Spark用来<strong>处理结构化数据</strong>的一个模块，</p>
<p>它提供了2个编程抽象：<strong>DataFrame</strong>和<strong>DataSet</strong>，并且作为<strong>分布式SQL查询引擎</strong>的作用。</p>
<h3 id="1-1-特点"><a href="#1-1-特点" class="headerlink" title="1.1 特点"></a>1.1 特点</h3><ul>
<li>易整合</li>
<li>统一的数据访问方式</li>
<li>兼容Hive</li>
<li>标准的数据连接</li>
</ul>
<h3 id="1-2-DataFrame"><a href="#1-2-DataFrame" class="headerlink" title="1.2 DataFrame"></a>1.2 DataFrame</h3><p>DataFrame也是一个分布式数据容器。</p>
<p>但DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。</p>
<p>同时，与Hive类似，<strong>DataFrame也支持嵌套数据类型</strong>（struct、array和map）。</p>
<p>DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待，DataFrame也是懒执行的。</p>
<p> 性能上比RDD要高，主要原因：</p>
<p> 优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。</p>
<h3 id="1-3-DataSet"><a href="#1-3-DataSet" class="headerlink" title="1.3 DataSet"></a>1.3 DataSet</h3><p>DataSet是</p>
<p> 1)Dataframe API的一个扩展，是Spark最新的数据抽象。</p>
<p> 2)用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。</p>
<p> 3)Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</p>
<p> 4)样例类被用来在Dataset中定义数据的结构信息，<strong>样例类中每个属性的名称直接映射到DataSet中的字段名称</strong>。</p>
<p> 5)Dataframe是Dataset的特列，DataFrame&#x3D;Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。</p>
<p> 6)DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].</p>
<p> 7)DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。</p>
<h2 id="2-SparkSQL编程"><a href="#2-SparkSQL编程" class="headerlink" title="2. SparkSQL编程"></a>2. SparkSQL编程</h2><h3 id="2-1-SparkSession"><a href="#2-1-SparkSession" class="headerlink" title="2.1 SparkSession"></a>2.1 SparkSession</h3><p>在<strong>老spark的版本</strong>中，SparkSQL提供两种SQL查询起始点：一个叫<strong>SQLContext</strong>，用于Spark自己提供的SQL查询；一个叫<strong>HiveContext</strong>，用于连接Hive的查询。</p>
<p><strong>SparkSession</strong>是Spark最新的SQL查询起始点，实质上<strong>是SQLContext和HiveContext的组合</strong>，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。<strong>SparkSession内部封装了sparkContext</strong>，所以计算实际上是由sparkContext完成的。</p>
<h3 id="2-2-DataFrame（DataSet基本一致）"><a href="#2-2-DataFrame（DataSet基本一致）" class="headerlink" title="2.2 DataFrame（DataSet基本一致）"></a>2.2 DataFrame（DataSet基本一致）</h3><h4 id="2-2-1-创建"><a href="#2-2-1-创建" class="headerlink" title="2.2.1 创建"></a>2.2.1 创建</h4><p>在Spark SQL中SparkSession是创建DataFrame和执行SQL的入口，</p>
<p>创建DataFrame有三种方式：</p>
<ul>
<li>通过Spark的数据源进行创建</li>
<li>从一个存在的RDD进行转换</li>
<li>从Hive Table进行查询返回</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">1. 从Spark数据源进行创建</span><br><span class="line">1)查看Spark数据源进行创建的文件格式	</span><br><span class="line">	spark.read.	</span><br><span class="line">	csv   format   jdbc   json   load   option   options   orc	parquet   </span><br><span class="line">	schema   table   text   textFile</span><br><span class="line">	</span><br><span class="line">2)读取json文件创建DataFrame	</span><br><span class="line">	val df=spark.read.json(&quot;/opt/module/spark/.../people.json&quot;)</span><br><span class="line">	</span><br><span class="line">3）展示结果	df.show</span><br><span class="line"></span><br><span class="line">2. 从RDD进行转换</span><br><span class="line">注意：如果需要RDD与DF或者DS之间操作，那么都需要引入 import spark.implicits._  </span><br><span class="line">【spark不是包名，而是sparkSession对象的名称】</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">val peopleRDD = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;)</span><br><span class="line"></span><br><span class="line">1)手动转换</span><br><span class="line">peopleRDD.map&#123;x=&gt;    </span><br><span class="line">	val para = x.split(&quot;,&quot;);			  				 		、、、、、</span><br><span class="line">	(para(0),para(1).trim.toInt)	</span><br><span class="line">	&#125;.toDF(&quot;name&quot;,&quot;age&quot;)</span><br><span class="line">	</span><br><span class="line">2）通过反射确定（需要用到样例类）	</span><br><span class="line">	（1）创建一个样例类	</span><br><span class="line">		case class People(name:String, age:Int)	</span><br><span class="line">	（2）根据样例类将RDD转换为DataFrame	</span><br><span class="line">		peopleRDD.map&#123; x =&gt; 	</span><br><span class="line">			val para = x.split(&quot;,&quot;);	</span><br><span class="line">			People(para(0),para(1).trim.toInt)	</span><br><span class="line">		&#125;.toDF</span><br><span class="line">		</span><br><span class="line">3）通过编程的方式（--了解--）	</span><br><span class="line">	（1）导入所需的类型	</span><br><span class="line">		import org.apache.spark.sql.types._	</span><br><span class="line">	（2）创建Schema	</span><br><span class="line">		val structType: StructType = </span><br><span class="line">		StructType(StructField(&quot;name&quot;, StringType) </span><br><span class="line">			:: StructField(&quot;age&quot;, IntegerType) :: Nil)	</span><br><span class="line">			</span><br><span class="line">	（3）导入所需的类型	</span><br><span class="line">		import org.apache.spark.sql.Row	</span><br><span class="line">		</span><br><span class="line">	（4）根据给定的类型创建二元组RDD	</span><br><span class="line">		val data = peopleRDD.map&#123; x =&gt; 	</span><br><span class="line">			val para = x.split(&quot;,&quot;);	</span><br><span class="line">			Row(para(0),para(1).trim.toInt)</span><br><span class="line">		&#125;	</span><br><span class="line">		</span><br><span class="line">	（5）根据数据及给定的schema创建DataFrame	</span><br><span class="line">		val dataFrame = spark.createDataFrame(data, structType)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3. 从Hive Table进行查询返回</span><br></pre></td></tr></table></figure>

<h4 id="2-2-2-SQL风格语法"><a href="#2-2-2-SQL风格语法" class="headerlink" title="2.2.2 SQL风格语法"></a>2.2.2 SQL风格语法</h4><ol>
<li><p>创建DF</p>
<p>spark.read.json(“”)</p>
</li>
<li><p>对DataFrame创建一个临时表</p>
<p>df.createOrReplaceTempView(“people”)</p>
</li>
<li><p>通过SQL语句实现查询全表</p>
<p>val sqlDF &#x3D; spark.sql(“SELECT * FROM people”)</p>
</li>
<li><p>结果展示</p>
<p>sqlDF.show</p>
</li>
</ol>
<p>注意：临时表是Session范围内的，Session退出后，表就失效了。</p>
<p>如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，</p>
<p>如：global_temp.people</p>
<ol>
<li><p>对于DataFrame创建一个全局表</p>
<p>df.createGlobalTempView(“people”)</p>
</li>
<li><p>通过SQL语句实现查询全表</p>
<p>spark.sql(“SELECT * FROM global_temp.people”).show()</p>
<p>spark.newSession().sql(“SELECT * FROM global_temp.people”).show()</p>
</li>
</ol>
<h3 id="2-3-DataFrame与DataSet的互操作"><a href="#2-3-DataFrame与DataSet的互操作" class="headerlink" title="2.3 DataFrame与DataSet的互操作"></a>2.3 DataFrame与DataSet的互操作</h3><p>在使用一些特殊的操作时，一定要加上 import spark.implicits._</p>
<p>不然toDF、toDS无法使用。</p>
<ol>
<li><p>DataFrame转换为DataSe</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1）创建一个DateFrame</span><br><span class="line">val df = spark.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">2）创建一个样例类</span><br><span class="line">case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">3）将DateFrame转化为DataSet</span><br><span class="line">df.as[Person]</span><br></pre></td></tr></table></figure>
</li>
<li><p>DataSet转换为DataFrame</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1）创建一个样例类</span><br><span class="line">case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">2）创建DataSet</span><br><span class="line">val ds = Seq(Person(&quot;Andy&quot;, 32)).toDS()</span><br><span class="line"></span><br><span class="line">3）将DataSet转化为DataFrame</span><br><span class="line">val df = ds.toDF4)展示df.show</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-4-RDD、DataFrame、DataSet"><a href="#2-4-RDD、DataFrame、DataSet" class="headerlink" title="2.4 RDD、DataFrame、DataSet"></a>2.4 RDD、DataFrame、DataSet</h3><p>在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。</p>
<p>他们和RDD有什么区别呢？首先从版本的产生上来看：</p>
<p> RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt;Dataset(Spark1.6)</p>
<p>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。</p>
<p>在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。</p>

    </div>

    <div></div>
  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-SQL"><span class="toc-text">Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%A6%82%E8%BF%B0"><span class="toc-text">1. 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%89%B9%E7%82%B9"><span class="toc-text">1.1 特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-DataFrame"><span class="toc-text">1.2 DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-DataSet"><span class="toc-text">1.3 DataSet</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-SparkSQL%E7%BC%96%E7%A8%8B"><span class="toc-text">2. SparkSQL编程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-SparkSession"><span class="toc-text">2.1 SparkSession</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-DataFrame%EF%BC%88DataSet%E5%9F%BA%E6%9C%AC%E4%B8%80%E8%87%B4%EF%BC%89"><span class="toc-text">2.2 DataFrame（DataSet基本一致）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-%E5%88%9B%E5%BB%BA"><span class="toc-text">2.2.1 创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-SQL%E9%A3%8E%E6%A0%BC%E8%AF%AD%E6%B3%95"><span class="toc-text">2.2.2 SQL风格语法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-DataFrame%E4%B8%8EDataSet%E7%9A%84%E4%BA%92%E6%93%8D%E4%BD%9C"><span class="toc-text">2.3 DataFrame与DataSet的互操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-RDD%E3%80%81DataFrame%E3%80%81DataSet"><span class="toc-text">2.4 RDD、DataFrame、DataSet</span></a></li></ol></li></ol></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by/4.0/">知识共享署名 4.0 国际许可协议</a>
    <span>进行许可。 转载时请注明原文链接。</span>
</div>
<div class="share" style="width: 100%;">
  <img src="/../../../../images/wechat.png" alt="Running Geek" style="margin: auto; display: block;"/>

  <div style="margin: auto; text-align: center; font-size: 0.8em; color: grey;">微信扫一扫向我投食</div>
  
</div>

  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/2018/01/01/Spark/SparkGraph/" rel="next" title="Spark Graph">
          Spark Graph
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/2018/01/01/a.Template/" rel="prev" title="template">
            template
          </a>
          <span>〉</span>
        
      </div>
    </div>
  


    </div>

    

  </div>
  <footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" target="_blank" rel="noopener" href="https://zshlovely.github.io/">首页</a> |
        <a class="bottom-item" href="https://zshlovely.github.io/" target="_blank">主站</a> |
        <a class="bottom-item" href="https://github.com/fenghuayangyi" target="_blank">GitHub</a>
    </div>
</footer>
  

<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>

  



</body>
</html>
