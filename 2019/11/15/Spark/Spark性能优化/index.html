
<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
  
  
    <meta name="keywords" content="Spark," />
  

  
    <meta name="description" content="既然选择远方，便只顾风雨兼程" />
  
  
  <link rel="icon" type="image/x-icon" href="/logo.png">
  <title>Spark性能优化 [ shaohua&#39;s blog ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  <div class="nav-container">
    <nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    <img class="avatar" src="/images/logo.png">
    <span class="title">shaohua&#39;s blog</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">归档</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/tags" class="pure-menu-link">标签</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/categories" class="pure-menu-link">分类</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
          
      
  </ul>
   
</nav>
  </div>

  <div class="container" id="content-outer">
    <div class="inner" id="content-inner">
      <div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        Spark性能优化
      </h1>
      <span>
        
        <time class="time" datetime="2019-11-15T01:46:00.000Z">
        2019-11-15
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">
      <span id="busuanzi_value_page_pv"></span> 点击
    </span>
      <span class="slash">/</span>
      <span class="read">阅读耗时 63 分钟</span>
    </header>

    <div class="post-content">
      <h1 id="Spark性能优化技术概述"><a href="#Spark性能优化技术概述" class="headerlink" title="Spark性能优化技术概述"></a>Spark性能优化技术概述</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1、参数调优</span><br><span class="line"></span><br><span class="line">2、代码调优</span><br><span class="line"></span><br><span class="line">3、使用高性能序列化类库(Kryo序列化机制)</span><br><span class="line"></span><br><span class="line">4、数据本地化</span><br><span class="line"></span><br><span class="line">5、内存调优</span><br><span class="line"></span><br><span class="line">6、Spark shuffle 调优（核心中的核心，重中之重）</span><br><span class="line"></span><br><span class="line">7、Spark数据倾斜问题治理</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="1-Spark参数调优"><a href="#1-Spark参数调优" class="headerlink" title="1. Spark参数调优"></a>1. Spark参数调优</h1><h3 id="资源调优"><a href="#资源调优" class="headerlink" title="资源调优"></a>资源调优</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">提交命令选项：（在提交Application的时候使用选项）</span><br><span class="line">--driver-memory 4g : driver内存大小，一般没有广播变量(broadcast)时，</span><br><span class="line">设置4g足够，如果有广播变量，视情况而定，可设置6G，8G，12G等均可</span><br><span class="line"></span><br><span class="line">--executor-memory 4g : 每个executor的内存，正常情况下是4g足够，但有时</span><br><span class="line">处理大批量数据时容易内存不足，再多申请一点，如6G</span><br><span class="line"></span><br><span class="line">--num-executors 15 : 总共申请的executor数目，普通任务十几个或者几十个</span><br><span class="line">足够了，若是处理海量数据如百G上T的数据时可以申请多一些，100，200等</span><br><span class="line"></span><br><span class="line">--executor-cores 2  : 每个executor内的核数，即每个executor中的任务</span><br><span class="line">task数目，此处设置为2，即2个task共享上面设置的6g内存，每个map或reduce</span><br><span class="line">任务的并行度是executor数目*executor中的任务数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">配置信息：（Application的代码中设置或在Spark-default.conf中设置）</span><br><span class="line">spark.driver.memory</span><br><span class="line">spark.executor.cores</span><br><span class="line">spark.executor.memory</span><br><span class="line">spark.max.cores</span><br><span class="line">spark.driver.maxResultSize</span><br><span class="line">spark.driver.extraJavaOptions=-Xss100m  //设置jvm栈大小</span><br><span class="line">spark.executor.extraJavaOptions=-Xss100m  //设置jvm栈大小</span><br><span class="line">spark.kryoserializer.buffer.max=128m</span><br><span class="line">spark.kryoserializer.buffer=64m </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">动态资源分配：</span><br><span class="line">spark.dynamicAllocation.enabled true //开启动态资源分配</span><br><span class="line">spark.dynamicAllocation.minExecutors 1 //每个Application最小分配的executor数</span><br><span class="line">spark.dynamicAllocation.maxExecutors 30 //每个Application最大并发分配的executor数</span><br><span class="line">spark.shuffle.service.enabled true //启用External shuffle Service服务</span><br><span class="line">spark.shuffle.service.port 7337 //Shuffle Service服务端口，必须和yarn-site中的一致</span><br><span class="line">spark.dynamicAllocation.schedulerBacklogTimeout 1s</span><br><span class="line">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s</span><br></pre></td></tr></table></figure>

<h3 id="并行度调节参数"><a href="#并行度调节参数" class="headerlink" title="并行度调节参数"></a>并行度调节参数</h3><p>并行度的合理调整，可以降低资源浪费提高spark任务的运行效率。</p>
<p>task的数量应该设置为sparkCPU cores的2-3倍（不考虑数据问题的情况下）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">—-spark.default.parallelism 200 ：Spark作业的默认为500~1000个比较</span><br><span class="line">合适,如果不设置，spark会根据底层HDFS的block数量设置task的数量，这样</span><br><span class="line">会导致并行度偏少，资源利用不充分。该参数设为num-executors * </span><br><span class="line">executor-cores的2~3倍比较合适。</span><br><span class="line"></span><br><span class="line">--spark.sql.shuffle.partitions 200 : sparksql并行度调节</span><br><span class="line"></span><br><span class="line">-- spark.storage.memoryFraction 0.6 : 设置RDD持久化数据在Executor</span><br><span class="line">内存中能占的最大比例。默认值是0.6</span><br><span class="line"></span><br><span class="line">-- spark.storage.memoryFraction 0.6 : 设置RDD持久化数据在Executor</span><br><span class="line">内存中能占的最大比例。默认值是0.6</span><br><span class="line"></span><br><span class="line">—-spark.shuffle.memoryFraction 0.2 ：设置shuffle过程中一个task拉取</span><br><span class="line">到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，</span><br><span class="line">默认是0.2，如果shuffle聚合时使用的内存超出了这个20%的限制，多余数据会</span><br><span class="line">被溢写到磁盘文件中去，降低shuffle性能</span><br><span class="line"></span><br><span class="line">—-spark.yarn.executor.memoryOverhead 1G ：executor执行的时候，用的</span><br><span class="line">内存可能会超过executor-memory，所以会为executor额外预留一部分内存，</span><br><span class="line">spark.yarn.executor.memoryOverhead即代表这部分内存</span><br></pre></td></tr></table></figure>

<p>此外，除了调节参数进行并行度调优， 还可以调优 <em><strong>自定义分区器</strong></em></p>
<h1 id="2-Spark参数总结"><a href="#2-Spark参数总结" class="headerlink" title="2. Spark参数总结"></a>2. Spark参数总结</h1><p>参考文献：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zyzzxycj/article/details/81011540">https://blog.csdn.net/zyzzxycj/article/details/81011540</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/hithink/p/9858206.html">https://www.cnblogs.com/hithink/p/9858206.html</a></p>
<p>因为每个集群规模都不一样，只有理解了参数的用途，调试出符合自己业务场景集群环境，并且能在扩大集群、业务的情况下，能够跟着修改参数才算是正确的参数调优。我的目标就是能够熟练的进行参数调优。</p>
<h2 id="1-Application-Properties-应用基本属性"><a href="#1-Application-Properties-应用基本属性" class="headerlink" title="1*. Application Properties 应用基本属性"></a>1*. Application Properties 应用基本属性</h2><p>这些参数比较重要，执行spark的shell 一般都需要配置一下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.cores  </span><br><span class="line">	driver端分配的核数，默认为1,资源充足的话可以尽量给多。</span><br><span class="line">spark.driver.memory</span><br><span class="line">	driver端分配的内存数，默认为1g，资源充足的话可以尽量给多。</span><br><span class="line">spark.executor.memory</span><br><span class="line">	每个executor分配的内存数，默认1g，会受到yarn CDH的限制，和memoryOverhead相加 不能超过总内存限制。</span><br><span class="line">spark.driver.maxResultSize</span><br><span class="line">	driver端接收的最大结果大小，默认1GB，最小1MB，设置0为无限。</span><br><span class="line">	这个参数不建议设置的太大,如果要做数据可视化,更应该控制在20-30MB以内,过大会导致OOM(out of memory)。</span><br><span class="line">spark.extraListeners.(不常用)</span><br><span class="line">	默认none，随着SparkContext被创建而创建，用于监听单参数、无参数构造函数的创建，并抛出异常。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2-Runtime-Environment-运行环境"><a href="#2-Runtime-Environment-运行环境" class="headerlink" title="2. Runtime Environment 运行环境"></a>2. <strong>Runtime Environment 运行环境</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">主要是一些日志，jvm参数的额外配置，jars等一些自定义的配置(待补充)</span><br></pre></td></tr></table></figure>

<h2 id="3-Shuffle-Behavior"><a href="#3-Shuffle-Behavior" class="headerlink" title="3. Shuffle Behavior"></a>3. Shuffle Behavior</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">spark.reducer.maxSizeInFlight</span><br><span class="line">	默认48m。从每个reduce任务同时拉取的最大map数，每个reduce都会在完成任务后，需要一个堆外内存的缓冲区来存放结果，如果没有充裕的内存就尽可能把这个调小一点。。相反，堆外内存充裕，调大些就能节省gc时间。</span><br><span class="line">spark.reducer.maxBlocksInFlightPerAddress(一般不会改)</span><br><span class="line">	限制了每个主机每次reduce可以被多少台远程主机拉取文件块，调低这个参数可以有效减轻node manager的负载。（默认值Int.MaxValue）</span><br><span class="line">spark.reducer.maxReqsInFlight</span><br><span class="line">	限制远程机器拉取本机器文件块的请求数，随着集群增大，需要对此做出限制。否则可能会使本机负载过大而挂掉。（默认值为Int.MaxValue）</span><br><span class="line">spark.reducer.maxReqSizeShuffleToMem</span><br><span class="line">	shuffle请求的文件块大小 超过这个参数值，就会被强行落盘，防止一大堆并发请求把内存占满。（默认Long.MaxValue）</span><br><span class="line">spark.shuffle.compress</span><br><span class="line">	是否压缩map输出文件，默认压缩 true</span><br><span class="line">spark.shuffle.spill.compress</span><br><span class="line">	shuffle过程中溢出的文件是否压缩，默认true，使用spark.io.compression.codec压缩。</span><br><span class="line">spark.shuffle.file.buffer</span><br><span class="line">	在内存输出流中 每个shuffle文件占用内存大小，适当提高 可以减少磁盘读写 io次数，初始值为32k。</span><br><span class="line">spark.shuffle.memoryFraction</span><br><span class="line">	该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</span><br><span class="line">	cache少且内存充足时，可以调大该参数，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。</span><br><span class="line">spark.shuffle.manager</span><br><span class="line">	当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），	 则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但	 是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</span><br><span class="line">	当使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read 	task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这	  种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</span><br><span class="line">spark.shuffle.consolidateFiles</span><br><span class="line">	如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并	 shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开	 销，提升性能。</span><br><span class="line">spark.shuffle.io.maxRetries（重试次数）</span><br><span class="line">	shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，	  是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导	致作业执行失败。	</span><br><span class="line">spark.shuffle.io.retryWait</span><br><span class="line">	同上，默认5s，建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</span><br><span class="line"></span><br><span class="line">spark.io.encryption.enabled</span><br><span class="line">spark.io.encryption.keySizeBits</span><br><span class="line">spark.io.encryption.keygen.algorithm</span><br><span class="line">	io加密，默认关闭</span><br></pre></td></tr></table></figure>

<h2 id="4-Spark-UI"><a href="#4-Spark-UI" class="headerlink" title="4. Spark UI"></a>4. Spark UI</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这一块配置，是有关于spark日志的。日志开关，日志输出路径，是否压缩。(待补充)</span><br></pre></td></tr></table></figure>

<h2 id="5-Compression-and-Serialization压缩与序列化"><a href="#5-Compression-and-Serialization压缩与序列化" class="headerlink" title="5. Compression and Serialization压缩与序列化"></a>5. Compression and Serialization压缩与序列化</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spark.broadcast.compress</span><br><span class="line">	广播变量前是否会先进行压缩。默认true （spark.io.compression.codec）</span><br><span class="line">spark.io.compression.codec</span><br><span class="line">	压缩RDD数据、日志、shuffle输出等的压缩格式 默认lz4</span><br><span class="line">spark.io.compression.lz4.blockSize</span><br><span class="line">	使用lz4压缩时，每个数据块大小 默认32k</span><br><span class="line">spark.rdd.compress</span><br><span class="line">	rdd是否压缩 默认false，节省memory_cache大量内存 消耗更多的cpu资源（时间）。</span><br><span class="line">spark.serializer.objectStreamReset</span><br><span class="line">	当使用JavaSerializer序列化时，会缓存对象防止写多余的数据，但这些对象就不会被gc，可以输入reset 清空缓存。默认缓存100个对象，修改成-1则不缓存任何对象。</span><br></pre></td></tr></table></figure>

<p>压缩以后节省内存资源，消耗cpu资源。</p>
<h2 id="6-Memory-Management-内存管理"><a href="#6-Memory-Management-内存管理" class="headerlink" title="6. Memory Management 内存管理"></a>6. <strong>Memory Management 内存管理</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">spark.memory.fraction</span><br><span class="line">	执行内存和缓存内存（堆）占jvm总内存的比例，剩余的部分是spark留给用户存储内部源数据、数据结构、异常大	  的结果数据。默认值0.6，调小会导致频繁gc，调大容易造成oom。</span><br><span class="line">spark.memory.storageFraction</span><br><span class="line">	用于存储的内存在堆中的占比，默认0.5。调大会导致执行内存过小，执行数据落盘，影响效率；调小会导致缓存	  	  内存不够，缓存到磁盘上去，影响效率。</span><br><span class="line">	另外，执行内存和缓存内存公用java堆，当执行内存没有使用时，会动态分配给缓存内存使用，反之也是这样。如	果执行内存不够用，可以将存储内存释放移动到磁盘上（最多释放不能超过本参数划分的比例），但存储内存不能	  把执行内存抢走。</span><br><span class="line">spark.memory.offHeap.enabled</span><br><span class="line">	是否允许使用堆外内存来进行某些操作。默认false</span><br><span class="line">spark.memory.offHeap.size</span><br><span class="line">	允许使用进行操作的堆外内存的大小，单位bytes 默认0</span><br><span class="line">spark.storage.replication.proactive</span><br><span class="line">	针对失败的executor，主动去cache 有关的RDD中的数据。默认false</span><br><span class="line">spark.cleaner.periodicGC.interval</span><br><span class="line">	控制触发gc的频率，默认30min</span><br><span class="line">spark.cleaner.referenceTracking</span><br><span class="line">	是否进行context cleaning，默认true</span><br><span class="line">spark.cleaner.referenceTracking.blocking</span><br><span class="line">	清理线程是否应该阻止清理任务，默认true</span><br><span class="line">spark.cleaner.referenceTracking.blocking.shuffle</span><br><span class="line">	清理线程是否应该阻止shuffle的清理任务，默认false</span><br><span class="line">spark.cleaner.referenceTracking.cleanCheckpoints</span><br><span class="line">	清理线程是否应该清理依赖超出范围的检查点文件（checkpoint files不知道怎么翻译。。）默认false</span><br></pre></td></tr></table></figure>

<h2 id="7-Executor-behavior"><a href="#7-Executor-behavior" class="headerlink" title="7*. Executor behavior"></a>7*. Executor behavior</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">spark.broadcast.blockSize</span><br><span class="line">	TorrentBroadcastFactory中的每一个block大小，默认4m</span><br><span class="line">	过大会减少广播时的并行度，过小会导致BlockManager 产生 performance hit.（暂时没懂这是干啥用的）</span><br><span class="line">spark.executor.cores</span><br><span class="line">	每个executor的核数，默认yarn下1核，standalone下为所有可用的核。</span><br><span class="line">spark.default.parallelism</span><br><span class="line">	默认RDD的分区数、并行数。</span><br><span class="line">	像reduceByKey和join等这种需要分布式shuffle的操作中，最大父RDD的分区数；</span><br><span class="line">	像parallelize之类没有父RDD的操作，则取决于运行环境下得cluster manager：</span><br><span class="line">		如果为单机模式，本机核数；</span><br><span class="line">		集群模式为所有executor总核数与2?中最大的一个。</span><br><span class="line">spark.executor.heartbeatInterval</span><br><span class="line">	executor和driver心跳发送间隔，默认10s，必须远远小于spark.network.timeout</span><br><span class="line">spark.files.fetchTimeout</span><br><span class="line">	从driver端执行SparkContext.addFile() 抓取添加的文件的超时时间，默认60s</span><br><span class="line">spark.files.useFetchCache</span><br><span class="line">	默认true，如果设为true，拉取文件时会在同一个application中本地持久化，被若干个executors共享。这使得	   当同一个主机下有多个executors时，执行任务效率提高。</span><br><span class="line">spark.files.overwrite</span><br><span class="line">	默认false，是否在执行SparkContext.addFile() 添加文件时，覆盖已有的内容有差异的文件。</span><br><span class="line">spark.files.maxPartitionBytes</span><br><span class="line">	单partition中最多能容纳的文件大小，单位Bytes 默认134217728 (128 MB)</span><br><span class="line">spark.files.openCostInBytes</span><br><span class="line">	小文件合并阈值，小于该参数就会被合并到一个partition内。</span><br><span class="line">	默认4194304 (4 MB) 。这个参数在将多个文件放入一个partition时被用到，宁可设置的小一些，因为在		partition操作中，小文件肯定会比大文件快。</span><br><span class="line">spark.storage.memoryMapThreshold</span><br><span class="line">	从磁盘上读文件时，最小单位不能少于该设定值，默认2m，小于或者接近操作系统的每个page的大小。</span><br></pre></td></tr></table></figure>

<h2 id="8-Networking网络"><a href="#8-Networking网络" class="headerlink" title="8. Networking网络"></a>8. Networking网络</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">网络超时或者ip端口的一些配置</span><br></pre></td></tr></table></figure>

<h2 id="9-Scheduling调度"><a href="#9-Scheduling调度" class="headerlink" title="9. Scheduling调度"></a>9. Scheduling调度</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">spark.scheduler.maxRegisteredResourcesWaitingTime</span><br><span class="line">	在执行前最大等待申请资源的时间，默认30s。</span><br><span class="line">spark.scheduler.minRegisteredResourcesRatio</span><br><span class="line">	实际注册的资源数占预期需要的资源数的比例，默认0.8</span><br><span class="line">spark.scheduler.mode其他进阶的配置如下：</span><br><span class="line">	</span><br><span class="line">	调度模式，默认FIFO 先进队列先调度，可以选择FAIR。</span><br><span class="line">spark.scheduler.revive.interval</span><br><span class="line">	work回复重启的时间间隔，默认1s</span><br><span class="line">spark.scheduler.listenerbus.eventqueue.capacity</span><br><span class="line">	spark事件监听队列容量，默认10000，必须为正值，增加可能会消耗更多内存</span><br><span class="line">spark.blacklist.enabled***************************************************************</span><br><span class="line">	是否列入黑名单，默认false。如果设成true，当一个executor失败好几次时，会被列入黑名单，防止后续task		派发到这个executor。可以进一步调节spark.blacklist以下相关的参数：</span><br><span class="line">	（均为测试参数 Experimental）</span><br><span class="line">	spark.blacklist.timeout</span><br><span class="line">    spark.blacklist.task.maxTaskAttemptsPerExecutor</span><br><span class="line">    spark.blacklist.task.maxTaskAttemptsPerNode</span><br><span class="line">    spark.blacklist.stage.maxFailedTasksPerExecutor</span><br><span class="line">    spark.blacklist.application.maxFailedExecutorsPerNode</span><br><span class="line">    spark.blacklist.killBlacklistedExecutors</span><br><span class="line">    spark.blacklist.application.fetchFailure.enabled</span><br><span class="line">spark.speculation************************************************************************</span><br><span class="line">	推测，如果有task执行的慢了，就会重新执行它。默认false.</span><br><span class="line">	详细相关配置如下：</span><br><span class="line">	spark.speculation.interval</span><br><span class="line">		检查task快慢的频率，推测间隔，默认100ms。</span><br><span class="line">	spark.speculation.multiplier</span><br><span class="line">		推测比均值慢几次算是task执行过慢，默认1.5.</span><br><span class="line">	spark.speculation.quantile</span><br><span class="line">		在某个stage，完成度必须达到该参数的比例，才能被推测，默认0.75</span><br><span class="line">spark.task.cpus</span><br><span class="line">	每个task分配的cpu数，默认1</span><br><span class="line">spark.task.maxFailures</span><br><span class="line">	在放弃这个job前允许的最大失败次数，默认4</span><br><span class="line">spark.task.reaper.enabled(原先有 job失败了但一直显示有task在running，总算找到这个参数了)********</span><br><span class="line">	赋予spark监控有权限去kill那些失效的task，默认false</span><br><span class="line"></span><br><span class="line">其他进阶的配置如下：</span><br><span class="line">	spark.task.reaper.pollingInterval</span><br><span class="line">		轮询被kill掉的task的时间间隔，如果还在running，就会打warn日志，默认10s。</span><br><span class="line">	spark.task.reaper.threadDump</span><br><span class="line">		线程回收是是否产生日志，默认true。</span><br><span class="line">	spark.task.reaper.killTimeout</span><br><span class="line">		当一个被kill的task过了多久还在running，就会把那个executor给kill掉，默认-1。</span><br><span class="line">	spark.stage.maxConsecutiveAttempts</span><br><span class="line">		在终止前，一个stage连续尝试次数，默认4。</span><br></pre></td></tr></table></figure>

<h2 id="10-Dynamic-Allocation动态分配"><a href="#10-Dynamic-Allocation动态分配" class="headerlink" title="10. Dynamic Allocation动态分配"></a>10. Dynamic Allocation动态分配</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.enabled(这个配置只能动态调整executor的数量)</span><br><span class="line">	是否开启动态资源配置，根据工作负载来衡量是否应该增加或减少executor，默认false</span><br><span class="line">以下相关参数：</span><br><span class="line">	spark.dynamicAllocation.minExecutors</span><br><span class="line">		动态分配最小executor个数，在启动时就申请好的，默认0</span><br><span class="line">	spark.dynamicAllocation.maxExecutors</span><br><span class="line">		动态分配最大executor个数，默认infinity</span><br><span class="line">	park.dynamicAllocation.initialExecutors**************************</span><br><span class="line">		动态分配初始executor个数默认值=spark.dynamicAllocation.minExecutors</span><br><span class="line">spark.dynamicAllocation.executorIdleTimeout</span><br><span class="line">	当某个executor空闲超过这个设定值，就会被kill，默认60s</span><br><span class="line">spark.dynamicAllocation.cachedExecutorIdleTimeout</span><br><span class="line">	当某个缓存数据的executor空闲时间超过这个设定值，就会被kill，默认infinity</span><br><span class="line">spark.dynamicAllocation.schedulerBacklogTimeout</span><br><span class="line">	任务队列非空，资源不够，申请executor的时间间隔，默认1s</span><br><span class="line">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout</span><br><span class="line">	同schedulerBacklogTimeout，是申请了新executor之后继续申请的间隔，默认=schedulerBacklogTimeout</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">	executor_memory=16G</span><br><span class="line">driver_memory=16G</span><br><span class="line">executor_cores=1</span><br><span class="line">partition_number=256</span><br><span class="line"></span><br><span class="line">--conf spark.dynamicAllocation.enabled=true \</span><br><span class="line">--conf spark.shuffle.service.enabled=true \</span><br><span class="line">--conf spark.dynamicAllocation.minExecutors=30 \</span><br><span class="line">--conf spark.dynamicAllocation.maxExecutors=60 \</span><br></pre></td></tr></table></figure>

<h2 id="11-Spark-Streaming"><a href="#11-Spark-Streaming" class="headerlink" title="11. Spark Streaming"></a>11. <strong>Spark Streaming</strong></h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zyzzxycj/article/details/82428031">https://blog.csdn.net/zyzzxycj/article/details/82428031</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Spark Streaming</span><br></pre></td></tr></table></figure>



<h1 id="3-Spark-编程建议（代码调优）"><a href="#3-Spark-编程建议（代码调优）" class="headerlink" title="3. Spark 编程建议（代码调优）"></a>3. Spark 编程建议（代码调优）</h1><h3 id="1-使用广播变量"><a href="#1-使用广播变量" class="headerlink" title="1.使用广播变量"></a>1.使用广播变量</h3><p>开发过程中，会遇到 <strong>需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合）</strong>，那么此时就应该使用Spark的广播(Broadcast）功能来提升性能，函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。如果使用的外部变量比较大，建议 <strong>使用Spark的广播功能</strong>，对该变量进行广播。广播后的变量，<strong>会保证每个Executor的内存中，只驻留一份变量副本</strong> ，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p>
<p>广播大变量发送方式：Executor一开始并没有广播变量，而是task运行需要用到广播变量，会找executor的blockManager要，bloackManager找Driver里面的blockManagerMaster要。</p>
<p>使用广播变量可以大大降低集群中变量的副本数。不使用广播变量，变量的副本数和task数一致。使用广播变量变量的副本和Executor数一致。</p>
<h3 id="2-避免重复创建RDD，尽量复用同一份数据。"><a href="#2-避免重复创建RDD，尽量复用同一份数据。" class="headerlink" title="2.避免重复创建RDD，尽量复用同一份数据。"></a>2.避免重复创建RDD，尽量复用同一份数据。</h3><p>一般选择cache,或者persist(MEMORY_AND_DISK_SER)</p>
<ul>
<li><p>如果有RDD复用，尤其是该RDD需要花费比较长的时间，建议对该RDD做cache，若该RDD每个partition需要消耗很多内存，建议开启Kryo序列化机制(据说可节省2到5倍空间),若还是有比较大的内存开销，可将storage_level设置为MEMORY_AND_DISK_SER</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">https://www.cnblogs.com/jinniezheng/p/8548251.html    Kryo序列化机制</span><br><span class="line"></span><br><span class="line">持久化算子(懒加载)</span><br><span class="line">cache:</span><br><span class="line">	MEMORY_ONLY</span><br><span class="line">persist：</span><br><span class="line">	MEMORY_ONLY</span><br><span class="line">	MEMORY_ONLY_SER</span><br><span class="line">	MEMORY_AND_DISK_SER</span><br></pre></td></tr></table></figure>

</li>
<li><p>使用checkpoint</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.如果一个RDD的计算时间比较长或者计算起来比较复杂，一般将这个RDD的计算结果保存到HDFS上，这样数据会更加安全。</span><br><span class="line"></span><br><span class="line">2.如果一个RDD的依赖关系非常长，也会使用checkpoint, 会切断依赖关系，提高容错的效率。</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-尽量避免使用shuffle类算子"><a href="#3-尽量避免使用shuffle类算子" class="headerlink" title="3.尽量避免使用shuffle类算子"></a>3.尽量避免使用shuffle类算子</h3><p>因为shuffle操作是spark中最消耗性能的地方，reduceByKey、join、distinct、repartition等算子都会触发shuffle操作，尽量使用map类的非shuffle算子</p>
<ul>
<li>如果2个RDD做join，其中一个数据量很小，可以采用Broadcast Join，将小的RDD数据collect到driver内存中，将其BroadCast到另外以RDD中，其他场景想优化后面会讲</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 使用广播变量来模拟使用join,使用情况：一个RDD比较大，一个RDD比较小（条件）。</span><br><span class="line">join算子=广播变量+filter、广播变量+map、广播变量+flatMap</span><br><span class="line"></span><br><span class="line">2. 一定要用join的话 考虑是否可以附带使用布隆过滤器</span><br></pre></td></tr></table></figure>

<ul>
<li><p>若需要Broadcast一个大的对象到远端作为字典查询，可使用多executor-cores，大executor-memory。若将该占用内存较大的对象存储到外部系统，executor-cores&#x3D;1， executor-memory&#x3D;m(默认值2g),可以正常运行，那么当大字典占用空间为size(g)时，executor-memory为2*size，executor-cores&#x3D;size&#x2F;m(向上取整)</p>
</li>
<li><p>如果对象太大无法BroadCast到远端，且需求是根据大的RDD中的key去索引小RDD中的key，可使用zipPartitions以 hash join 的方式实现，具体原理参考下一节的shuffle过程</p>
</li>
<li><p>用 aggregateByKey 和 reduceByKey 替代groupByKey,因为前两个是<strong>预聚合操作</strong>，会在每个节点本地对相同的key做聚合，等其他节点拉取所有节点上相同的key时，会大大减少磁盘IO以及网络开销。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">使用map-side预聚合的shuffle操作, 即尽量使用有combiner的shuffle类算子。</span><br><span class="line"></span><br><span class="line">combiner概念：</span><br><span class="line">	在map端，每一个map task计算完毕后进行的局部聚合。</span><br><span class="line"></span><br><span class="line">combiner好处：</span><br><span class="line">1) 降低shuffle write写磁盘的数据量。</span><br><span class="line">2) 降低shuffle read拉取数据量的大小。</span><br><span class="line">3) 降低reduce端聚合的次数。</span><br><span class="line"></span><br><span class="line">有combiner的shuffle类算子：</span><br><span class="line">1) reduceByKey:这个算子在map端是有combiner的，在一些场景中可以使用reduceByKey代替groupByKey。</span><br><span class="line">2) aggregateByKey</span><br><span class="line">3) combinerByKey</span><br></pre></td></tr></table></figure>


</li>
<li><p>使用repartition和coalesce算子操作分区。</p>
</li>
<li><p>repartition适用于RDD[V],  partitionBy适用于RDD[K, V]</p>
</li>
<li><p>mapPartitions操作替代普通map，foreachPartitions替代foreach</p>
</li>
<li><p>filter操作之后进行coalesce操作，可以减少RDD的partition数量</p>
</li>
<li><p>使用repartitionAndSortWithinPartitions替代repartition与sort类操作, 这样比分解操作效率高，因为它可以一边shuffle一边排序</p>
</li>
<li><p>尽量避免在一个Transformation中处理所有的逻辑，尽量分解成map、filter之类的操作</p>
</li>
<li><p>多个RDD进行union操作时，避免使用rdd.union(rdd).union(rdd).union(rdd)这种多重union，rdd.union只适合2个RDD合并，合并多个时采用SparkContext.union(Array(RDD))，避免union嵌套层数太多，导致的调用链路太长，耗时太久，且容易引发StackOverFlow</p>
</li>
<li><p>spark中的Group &#x2F; join &#x2F;XXXByKey &#x2F; distinct 等操作，都可以指定partition的个数，不需要额外使用repartition和partitionBy函数</p>
</li>
<li><p>尽量保证每轮Stage里每个task处理的数据量&gt;128M</p>
</li>
<li><p>2个RDD做笛卡尔积时，把小的RDD作为参数传入，如BigRDD.certesian(smallRDD)</p>
</li>
</ul>
<h3 id="4-优化数据结构"><a href="#4-优化数据结构" class="headerlink" title="4.优化数据结构"></a>4.优化数据结构</h3><p>java中有三种类型比较消耗内存：</p>
<ol>
<li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li>
<li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li>
<li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li>
</ol>
<p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p>
<h3 id="5-使用高性能的库fastutil"><a href="#5-使用高性能的库fastutil" class="headerlink" title="5.使用高性能的库fastutil"></a>5.使用高性能的库fastutil</h3><p>fasteutil介绍：</p>
<p>fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；</p>
<p><strong>fastutil能够提供更小的内存占用，更快的存取速度</strong>；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于，fastutil集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度。</p>
<p>fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。</p>
<p>fastutil最新版本要求Java 7以及以上版本。</p>
<p>使用： 见RandomExtractCars.java类</p>
<h1 id="4-使用Kryo优化序列化性能"><a href="#4-使用Kryo优化序列化性能" class="headerlink" title="4.使用Kryo优化序列化性能"></a>4.使用Kryo优化序列化性能</h1><p>在Spark中，主要有三个地方涉及到了序列化：</p>
<ol>
<li><p>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输。</p>
</li>
<li><p>将自定义的类型作为RDD的泛型类型时（比如JavaRDD<Student>，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求<strong>自定义的类必须实现Serializable接口</strong>。</p>
</li>
<li><p>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</p>
</li>
</ol>
<p>Kryo序列化器介绍：</p>
<p>Spark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1&#x2F;10。所以Kryo序列化优化以后，可以让网络传输的数据变少；在集群中耗费的内存资源大大减少。 </p>
<p><strong>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能</strong>。<em>Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream&#x2F;ObjectInputStream API来进行序列化和反序列化</em>。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p>
<p>Spark中使用Kryo：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Sparkconf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).registerKryoClasses(new Class[]&#123;Student.class&#125;)</span><br><span class="line"></span><br><span class="line">使用注册器</span><br><span class="line">public class KryoTest implements KryoRegistrator&#123;</span><br><span class="line">	@Override</span><br><span class="line">	public void registerClasses(Kryo kryo) &#123;</span><br><span class="line">		kryo.register(需要注册的类.class);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="5-数据本地化"><a href="#5-数据本地化" class="headerlink" title="5.数据本地化"></a>5.数据本地化</h1><p>数据本地化的级别：</p>
<ul>
<li><p>PROCESS_LOCAL</p>
<p>task要计算的数据在本进程（Executor）的内存中。</p>
</li>
<li><p>NODE_LOCAL</p>
<ul>
<li>task所计算的数据在本节点所在的磁盘上。</li>
<li>task所计算的数据在本节点其他Executor进程的内存中。</li>
</ul>
</li>
<li><p>NO_PREF</p>
<p>task所计算的数据在关系型数据库中，如mysql。</p>
</li>
<li><p>RACK_LOCAL</p>
<p>task所计算的数据在同机架的不同节点的磁盘或者Executor进程的内存中</p>
</li>
<li><p>ANY</p>
<p>跨机架。</p>
</li>
</ul>
<p>Spark中任务调度时，TaskScheduler在分发之前需要依据数据的位置来分发，最好将task分发到数据所在的节点上，如果TaskScheduler分发的task在默认3s依然无法执行的话，TaskScheduler会重新发送这个task到相同的Executor中去执行，会重试5次，如果依然无法执行，那么TaskScheduler会降低一级数据本地化的级别再次发送task。</p>
<p>会先尝试1,PROCESS_LOCAL数据本地化级别，如果重试5次每次等待3s,会默认这个Executor计算资源满了，那么会降低一级数据本地化级别到2，NODE_LOCAL,如果还是重试5次每次等待3s还是失败，那么还是会降低一级数据本地化级别到3，RACK_LOCAL。这样数据就会有网络传输，降低了执行效率。</p>
<h3 id="1-如何提高数据本地化的级别？"><a href="#1-如何提高数据本地化的级别？" class="headerlink" title="1.如何提高数据本地化的级别？"></a>1.如何提高数据本地化的级别？</h3><p>可以增加每次发送task的等待时间（默认都是3s），将3s倍数调大，结合WebUI来调节：</p>
<p>​	• spark.locality.wait </p>
<p>​	• spark.locality.wait.process</p>
<p>​	• spark.locality.wait.node</p>
<p>​	• spark.locality.wait.rack</p>
<p>注意：等待时间不能调大很大，调整数据本地化的级别不要本末倒置，虽然每一个task的本地化级别是最高了，但整个Application的执行时间反而加长。</p>
<h3 id="2-如何查看数据本地化的级别？"><a href="#2-如何查看数据本地化的级别？" class="headerlink" title="2.如何查看数据本地化的级别？"></a>2.如何查看数据本地化的级别？</h3><p>通过日志或者WebUI</p>
<h1 id="6-内存调优"><a href="#6-内存调优" class="headerlink" title="6.内存调优"></a>6.内存调优</h1><h2 id="1-spark内存模型-Executor内存模型"><a href="#1-spark内存模型-Executor内存模型" class="headerlink" title="1. spark内存模型(Executor内存模型)"></a>1. spark内存模型(Executor内存模型)</h2><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在worker节点的系统内存中开辟空间，进一步优化了内存的使用。</p>
<p>On-heap（堆内内存）:  jvm堆内存</p>
<p>Off-heap（堆外内存）：系统内存</p>
<p><img src="/../../../../images/spark_memory_model.png"></p>
<p><img src="D:/bona/WeCloudSync/myinsightbp/InitialOpera/MarkDown/img/spark_memory_model.png"></p>
<h3 id="1-堆内内存"><a href="#1-堆内内存" class="headerlink" title="1. 堆内内存"></a>1. 堆内内存</h3><p>堆内内存由spark.executor.memory 参数配置，同一个executor的多核共享堆内内存。</p>
<p>堆内内存 &#x3D; 存储内存（Storage memory） + 执行内存（Execution memory）+ other部分 + Reserved</p>
<ul>
<li>Storage用于缓存RDD数据和broadcast广播变量的内存使用</li>
<li>Execution仅提供shuffle过程的内存使用</li>
<li>Other部分提供Spark内部对象、用户自定义对象的内存空间</li>
</ul>
<h3 id="2-堆外内存"><a href="#2-堆外内存" class="headerlink" title="2. 堆外内存"></a>2. 堆外内存</h3><p>为了进一步优化内存的使用以及<strong>提高 Shuffle 时排序的效率</strong>，Spark 引入了堆外（Off-heap）内存，使之可以<strong>直接在worker节点的系统内存中开辟空间，存储经过序列化的二进制数据</strong>。</p>
<p>默认情况，堆外内存并不启用，spark.memory.offHeap.enabled 参数启用， spark.memory.offHeap.size 设置堆外内存大小。</p>
<p>堆外内存，用于 VM overheads, interned strings, other native overheads, etc。</p>
<h3 id="3-内存空间分配"><a href="#3-内存空间分配" class="headerlink" title="3. 内存空间分配"></a>3. 内存空间分配</h3><p>Spark 1.6 之后，内存模式（MemoryMode）默认为统一管理（Unified Memory Manager）方式，1.6 之前采用的静态管理（Static Memory Manager）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。</p>
<h4 id="1-静态内存管理（spark1-6之前）"><a href="#1-静态内存管理（spark1-6之前）" class="headerlink" title="1. 静态内存管理（spark1.6之前）"></a>1. 静态内存管理（spark1.6之前）</h4><h5 id="堆内存计算方式"><a href="#堆内存计算方式" class="headerlink" title="堆内存计算方式"></a>堆内存计算方式</h5><p>可用的存储内存 &#x3D; systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction<br>可用的执行内存 &#x3D; systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</p>
<p>上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险。（对于非序列化对象的内存采样估算会产生误差）</p>
<p><img src="/../../../../images/spark_memory_static_onheap.png"></p>
<p><img src="D:/bona/WeCloudSync/myinsightbp/InitialOpera/MarkDown/img/spark_memory_static_onheap.png"></p>
<h5 id="堆外空间的分配"><a href="#堆外空间的分配" class="headerlink" title="堆外空间的分配"></a>堆外空间的分配</h5><p>在默认情况下堆外内存并不启用，可通过配置 <code>spark.memory.offHeap.enabled</code> 参数启用，并由 <code>spark.memory.offHeap.size</code> 参数设定堆外空间的大小。</p>
<p>另外参数  <code>spark.executor.memoryOverhead</code>  和 <code>spark.memory.offHeap.size</code> 有啥区别？哪个才是真正控制堆外内存的？</p>
<p><strong>answer</strong>: <code>spark.memory.offHeap.size</code></p>
<p>总结如下（spark 2.X）：<br>spark.memory.offHeap.size 真正作用于spark executor的堆外内存<br>spark.executor.memoryOverhead 作用于yarn，通知yarn我要使用堆外内存和使用内存的大小，相当于spark.memory.offHeap.size +  spark.memory.offHeap.enabled，设置参数的大小并非实际使用内存大小。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
</tr>
</thead>
<tbody><tr>
<td>spark.executor.memoryOverhead</td>
<td>executorMemory * 0.10, with minimum of 384</td>
</tr>
<tr>
<td>spark.memory.offHeap.enabled</td>
<td>false</td>
</tr>
<tr>
<td>spark.memory.offHeap.size</td>
<td>0</td>
</tr>
</tbody></table>
<p><img src="/../../../../images/spark_memory_static_offheap.png"></p>
<p><img src="D:/bona/WeCloudSync/myinsightbp/InitialOpera/MarkDown/img/spark_memory_static_onheap.png"></p>
<p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，很容易造成存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p>
<h4 id="2-统一内存管理"><a href="#2-统一内存管理" class="headerlink" title="2. 统一内存管理"></a>2. 统一内存管理</h4><p>Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域。堆内内存以及堆外内存如下图所示。</p>
<p><img src="/../../../../images/spark_memory_unified_onheap.png"></p>
<p><img src="D:/bona/WeCloudSync/myinsightbp/InitialOpera/MarkDown/img/spark_memory_unified_offheap.png"></p>
<h5 id="动态占用机制（执行内存和存储内存）"><a href="#动态占用机制（执行内存和存储内存）" class="headerlink" title="动态占用机制（执行内存和存储内存）"></a>动态占用机制（执行内存和存储内存）</h5><p>动态占用机制，其规则如下：</p>
<ul>
<li>设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围</li>
<li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）</li>
<li>执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间</li>
<li>存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂</li>
</ul>
<p>凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。</p>
<p>无动态占用：</p>
<p>storage memory &#x3D; (spark.executor.memory - 300M) * spark.storage.memoryFraction * spark.storage.safetyFraction + 堆外内存</p>
<p>动态占用：</p>
<p>storage memory &#x3D; (spark.executor.memory - 300M) * spark.storage.memoryFraction + 堆外内存</p>
<h2 id="2-Spark内存管理"><a href="#2-Spark内存管理" class="headerlink" title="2. Spark内存管理"></a>2. Spark内存管理</h2><p>Spark 的存储内存和执行内存有着截然不同的管理方式：</p>
<p>对于存储内存来说，Spark 用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；</p>
<p>而对于执行内存，Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制。</p>
<h1 id="7-Spark-shuffle-调优"><a href="#7-Spark-shuffle-调优" class="headerlink" title="7.Spark shuffle 调优"></a>7.Spark shuffle 调优</h1><h1 id="8-数据倾斜问题"><a href="#8-数据倾斜问题" class="headerlink" title="8.数据倾斜问题"></a>8.数据倾斜问题</h1><h2 id="8-1-概述"><a href="#8-1-概述" class="headerlink" title="8.1 概述"></a>8.1 概述</h2><p>发生数据倾斜的现象</p>
<ul>
<li>整个stage绝大部分task很快就完成了，只有少数几个task,执行得很慢或者无法成功执行。</li>
<li>原本正常执行的job， 某天报错OOM（内存溢出）异常</li>
</ul>
<p>数据倾斜的原理</p>
<p>在进行shuffle的时候，action算子某个partition必须将各个节点上相同的key拉取到当前节点上的一个task来进行处理，比如按照key进行聚合或join等操作。如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。由于长尾效应，spark job运行进度是由运行时间最长的那个task决定的。</p>
<p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出,job无法完成。</p>
<p>查看导致数据倾斜的key的数据分布情况</p>
<ul>
<li><p>如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。</p>
</li>
<li><p>如果是对Spark RDD执行shuffle算子导致的数据倾斜，采样统计</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//pairs RDD 为倾斜rdd的 父rdd</span><br><span class="line">val sampledPairs = pairs.sample(false, 0.1)</span><br><span class="line">val sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">很多数据倾斜都是在数据的使用上造成的。我们举几个场景，并分别给出它们的解决方案。</span><br><span class="line"></span><br><span class="line">调参角度：</span><br><span class="line">1.通过增大 executor memory, 一般倾斜可以通过该方法解决，但是会降低任务的并行度</span><br><span class="line">2.增大 该stage的 partition</span><br><span class="line"></span><br><span class="line">解决思路：</span><br><span class="line">1.join 倾斜优化</span><br><span class="line"></span><br><span class="line">2.设置rdd压缩</span><br><span class="line">spark.rdd.compress（默认false）</span><br><span class="line"></span><br><span class="line">3.合理设置 executor/driver 的内存, 或者 stage对应的分区数</span><br></pre></td></tr></table></figure>

<h2 id="8-2-join-倾斜优化"><a href="#8-2-join-倾斜优化" class="headerlink" title="8.2 join 倾斜优化"></a>8.2 join 倾斜优化</h2><h3 id="8-2-1-broadcast-join"><a href="#8-2-1-broadcast-join" class="headerlink" title="8.2.1 broadcast join"></a>8.2.1 broadcast join</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果2个RDD做join，其中一个数据量很小，可以采用Broadcast Join，将小的RDD数据collect到driver内存中，将其BroadCast到另外以RDD中</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="8-2-2-布隆过滤器"><a href="#8-2-2-布隆过滤器" class="headerlink" title="8.2.2 布隆过滤器"></a>8.2.2 布隆过滤器</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="8-2-3-使用random-预join-即通过两次join解决"><a href="#8-2-3-使用random-预join-即通过两次join解决" class="headerlink" title="8.2.3 使用random 预join. 即通过两次join解决"></a>8.2.3 使用random 预join. 即通过两次join解决</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="8-2-4-采样-频次超高的broadcast-join，-其余正常join"><a href="#8-2-4-采样-频次超高的broadcast-join，-其余正常join" class="headerlink" title="8.2.4  采样 + 频次超高的broadcast join， 其余正常join"></a>8.2.4  采样 + 频次超高的broadcast join， 其余正常join</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="9-spark-hive-按行数分区"><a href="#9-spark-hive-按行数分区" class="headerlink" title="9. spark hive 按行数分区"></a>9. spark hive 按行数分区</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">-----------------------spark hive 按行数分区</span><br><span class="line">class LatestSessionIdLabelInfo(session_id: String,label: Array[Int],values: Map[Int, Double],multi_values: Map[Int, Array[String]],dir_label: Array[Int],version: String,source_id: Int,uid: Int,tp: String)</span><br><span class="line"></span><br><span class="line">val LATEST_TABLE_FILE_LINE = CacheConfUtils.getDouble(&quot;cdp.hive.latest.table.split.line.num&quot;, 2000000.0)</span><br><span class="line"></span><br><span class="line">//finalLatestLabelInfo: type为LatestSessionIdLabelInfo</span><br><span class="line">val pairFinalLatestLabelInfo = finalLatestLabelInfo</span><br><span class="line">            .map &#123; labelInfo =&gt;</span><br><span class="line">                val key = (labelInfo.version, labelInfo.source_id, labelInfo.uid, labelInfo.tp)</span><br><span class="line">                (key, labelInfo)</span><br><span class="line">            &#125;</span><br><span class="line">        pairFinalLatestLabelInfo.persist(StorageLevel.MEMORY_AND_DISK_SER)</span><br><span class="line"></span><br><span class="line">pairFinalLatestLabelInfo</span><br><span class="line">            .partitionBy(new LineCountSplitPartitioner(key2OffsetNumMap))</span><br><span class="line">            .map(_._2)</span><br><span class="line">            .toDS()</span><br><span class="line">            .write</span><br><span class="line">            .mode(SaveMode.Append)</span><br><span class="line">            .insertInto(latestLabelInfoTableName)</span><br><span class="line">        pairFinalLatestLabelInfo.unpersist()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//return (分区键, (offset, 该分区下文件数))</span><br><span class="line">def getKey2OffsetNumMap[T](keyCountMap: Map[T, Long], fileMaxLineCount: Double) = &#123;</span><br><span class="line"></span><br><span class="line">        val key2OffsetNumMap = new mutable.HashMap[T, (Int, Int)]</span><br><span class="line">        var offset = 0</span><br><span class="line">        keyCountMap</span><br><span class="line">            .mapValues(num =&gt; Math.ceil(num / fileMaxLineCount).toInt)</span><br><span class="line">            .foreach &#123; case (key, splitFileNum) =&gt;</span><br><span class="line">                // key 分区键</span><br><span class="line">                // offset: 在该key之前已存在多少个分区</span><br><span class="line">                // splitFileNum: 该key对应的分区下, 按照fileMaxLineCount切分后有多少个文件</span><br><span class="line">                key2OffsetNumMap.put(key, (offset, splitFileNum))</span><br><span class="line">                offset += splitFileNum</span><br><span class="line">            &#125;</span><br><span class="line">        key2OffsetNumMap</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 自定义分区器</span><br><span class="line"> * 按照行数切分分区</span><br><span class="line"> */</span><br><span class="line">class LineCountSplitPartitioner[T](keyPartNumMap: mutable.Map[T, (Int, Int)]) extends Partitioner &#123;</span><br><span class="line"></span><br><span class="line">    override def numPartitions: Int = keyPartNumMap.map &#123; case (key, (offset, splitFileNum)) =&gt; splitFileNum &#125;.sum</span><br><span class="line"></span><br><span class="line">    override def getPartition(key: Any): Int = &#123;</span><br><span class="line">        val (offset, splitFileNum) = keyPartNumMap(key.asInstanceOf[T])</span><br><span class="line">        val num = offset + (Math.abs(ThreadLocalRandom.current().nextInt()) % splitFileNum)</span><br><span class="line">        num</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="10-Spark问题记录"><a href="#10-Spark问题记录" class="headerlink" title="10.Spark问题记录"></a>10.Spark问题记录</h1>
    </div>

    <div></div>
  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E6%A6%82%E8%BF%B0"><span class="toc-text">Spark性能优化技术概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Spark%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="toc-text">1. Spark参数调优</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98"><span class="toc-text">资源调优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E5%BA%A6%E8%B0%83%E8%8A%82%E5%8F%82%E6%95%B0"><span class="toc-text">并行度调节参数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Spark%E5%8F%82%E6%95%B0%E6%80%BB%E7%BB%93"><span class="toc-text">2. Spark参数总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Application-Properties-%E5%BA%94%E7%94%A8%E5%9F%BA%E6%9C%AC%E5%B1%9E%E6%80%A7"><span class="toc-text">1*. Application Properties 应用基本属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Runtime-Environment-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83"><span class="toc-text">2. Runtime Environment 运行环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Shuffle-Behavior"><span class="toc-text">3. Shuffle Behavior</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Spark-UI"><span class="toc-text">4. Spark UI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Compression-and-Serialization%E5%8E%8B%E7%BC%A9%E4%B8%8E%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">5. Compression and Serialization压缩与序列化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Memory-Management-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">6. Memory Management 内存管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Executor-behavior"><span class="toc-text">7*. Executor behavior</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Networking%E7%BD%91%E7%BB%9C"><span class="toc-text">8. Networking网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Scheduling%E8%B0%83%E5%BA%A6"><span class="toc-text">9. Scheduling调度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-Dynamic-Allocation%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D"><span class="toc-text">10. Dynamic Allocation动态分配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-Spark-Streaming"><span class="toc-text">11. Spark Streaming</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Spark-%E7%BC%96%E7%A8%8B%E5%BB%BA%E8%AE%AE%EF%BC%88%E4%BB%A3%E7%A0%81%E8%B0%83%E4%BC%98%EF%BC%89"><span class="toc-text">3. Spark 编程建议（代码调优）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-text">1.使用广播变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%81%BF%E5%85%8D%E9%87%8D%E5%A4%8D%E5%88%9B%E5%BB%BARDD%EF%BC%8C%E5%B0%BD%E9%87%8F%E5%A4%8D%E7%94%A8%E5%90%8C%E4%B8%80%E4%BB%BD%E6%95%B0%E6%8D%AE%E3%80%82"><span class="toc-text">2.避免重复创建RDD，尽量复用同一份数据。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%B0%BD%E9%87%8F%E9%81%BF%E5%85%8D%E4%BD%BF%E7%94%A8shuffle%E7%B1%BB%E7%AE%97%E5%AD%90"><span class="toc-text">3.尽量避免使用shuffle类算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%BC%98%E5%8C%96%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-text">4.优化数据结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E4%BD%BF%E7%94%A8%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E5%BA%93fastutil"><span class="toc-text">5.使用高性能的库fastutil</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E4%BD%BF%E7%94%A8Kryo%E4%BC%98%E5%8C%96%E5%BA%8F%E5%88%97%E5%8C%96%E6%80%A7%E8%83%BD"><span class="toc-text">4.使用Kryo优化序列化性能</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E6%95%B0%E6%8D%AE%E6%9C%AC%E5%9C%B0%E5%8C%96"><span class="toc-text">5.数据本地化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E6%95%B0%E6%8D%AE%E6%9C%AC%E5%9C%B0%E5%8C%96%E7%9A%84%E7%BA%A7%E5%88%AB%EF%BC%9F"><span class="toc-text">1.如何提高数据本地化的级别？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E6%9C%AC%E5%9C%B0%E5%8C%96%E7%9A%84%E7%BA%A7%E5%88%AB%EF%BC%9F"><span class="toc-text">2.如何查看数据本地化的级别？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E5%86%85%E5%AD%98%E8%B0%83%E4%BC%98"><span class="toc-text">6.内存调优</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-spark%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B-Executor%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="toc-text">1. spark内存模型(Executor内存模型)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%A0%86%E5%86%85%E5%86%85%E5%AD%98"><span class="toc-text">1. 堆内内存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98"><span class="toc-text">2. 堆外内存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E5%88%86%E9%85%8D"><span class="toc-text">3. 内存空间分配</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88spark1-6%E4%B9%8B%E5%89%8D%EF%BC%89"><span class="toc-text">1. 静态内存管理（spark1.6之前）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A0%86%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="toc-text">堆内存计算方式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A0%86%E5%A4%96%E7%A9%BA%E9%97%B4%E7%9A%84%E5%88%86%E9%85%8D"><span class="toc-text">堆外空间的分配</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">2. 统一内存管理</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E5%8D%A0%E7%94%A8%E6%9C%BA%E5%88%B6%EF%BC%88%E6%89%A7%E8%A1%8C%E5%86%85%E5%AD%98%E5%92%8C%E5%AD%98%E5%82%A8%E5%86%85%E5%AD%98%EF%BC%89"><span class="toc-text">动态占用机制（执行内存和存储内存）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">2. Spark内存管理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Spark-shuffle-%E8%B0%83%E4%BC%98"><span class="toc-text">7.Spark shuffle 调优</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E9%97%AE%E9%A2%98"><span class="toc-text">8.数据倾斜问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-%E6%A6%82%E8%BF%B0"><span class="toc-text">8.1 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-join-%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96"><span class="toc-text">8.2 join 倾斜优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-1-broadcast-join"><span class="toc-text">8.2.1 broadcast join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-2-%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8"><span class="toc-text">8.2.2 布隆过滤器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-3-%E4%BD%BF%E7%94%A8random-%E9%A2%84join-%E5%8D%B3%E9%80%9A%E8%BF%87%E4%B8%A4%E6%AC%A1join%E8%A7%A3%E5%86%B3"><span class="toc-text">8.2.3 使用random 预join. 即通过两次join解决</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-4-%E9%87%87%E6%A0%B7-%E9%A2%91%E6%AC%A1%E8%B6%85%E9%AB%98%E7%9A%84broadcast-join%EF%BC%8C-%E5%85%B6%E4%BD%99%E6%AD%A3%E5%B8%B8join"><span class="toc-text">8.2.4  采样 + 频次超高的broadcast join， 其余正常join</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-spark-hive-%E6%8C%89%E8%A1%8C%E6%95%B0%E5%88%86%E5%8C%BA"><span class="toc-text">9. spark hive 按行数分区</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-Spark%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95"><span class="toc-text">10.Spark问题记录</span></a></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by/4.0/">知识共享署名 4.0 国际许可协议</a>
    <span>进行许可。 转载时请注明原文链接。</span>
</div>
<div class="share" style="width: 100%;">
  <img src="/../../../../images/wechat.png" alt="Running Geek" style="margin: auto; display: block;"/>

  <div style="margin: auto; text-align: center; font-size: 0.8em; color: grey;">微信扫一扫向我投食</div>
  
</div>

  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/2019/10/18/HBase/HBase%E5%9F%BA%E7%A1%80/" rel="next" title="HBase基础">
          HBase基础
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/2019/11/18/HBase/HBase%E7%AC%94%E8%AE%B0/" rel="prev" title="HBase笔记">
            HBase笔记
          </a>
          <span>〉</span>
        
      </div>
    </div>
  


    </div>

    

  </div>
  <footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" target="_blank" rel="noopener" href="https://zshlovely.github.io/">首页</a> |
        <a class="bottom-item" href="https://zshlovely.github.io/" target="_blank">主站</a> |
        <a class="bottom-item" href="https://github.com/fenghuayangyi" target="_blank">GitHub</a>
    </div>
</footer>
  

<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>

  



</body>
</html>
