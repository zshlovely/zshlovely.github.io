
<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
  
  
    <meta name="keywords" content="Hadoop," />
  

  
    <meta name="description" content="既然选择远方，便只顾风雨兼程" />
  
  
  <link rel="icon" type="image/x-icon" href="/logo.png">
  <title>Hadoop之MapReduce [ shaohua&#39;s blog ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  <div class="nav-container">
    <nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    <img class="avatar" src="/images/logo.png">
    <span class="title">shaohua&#39;s blog</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">归档</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/tags" class="pure-menu-link">标签</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/categories" class="pure-menu-link">分类</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
          
      
  </ul>
   
</nav>
  </div>

  <div class="container" id="content-outer">
    <div class="inner" id="content-inner">
      <div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        Hadoop之MapReduce
      </h1>
      <span>
        
        <time class="time" datetime="2019-04-11T17:10:20.000Z">
        2019-04-12
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">
      <span id="busuanzi_value_page_pv"></span> 点击
    </span>
      <span class="slash">/</span>
      <span class="read">阅读耗时 24 分钟</span>
    </header>

    <div class="post-content">
      <h1 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h1><h2 id="1-MapReduce分布式方案考虑的问题"><a href="#1-MapReduce分布式方案考虑的问题" class="headerlink" title="1. MapReduce分布式方案考虑的问题"></a>1. MapReduce分布式方案考虑的问题</h2><p> （1）运算逻辑要不要先分后合？</p>
<p> （2）程序如何分配运算任务（切片）？</p>
<p> （3）两阶段的程序如何启动？如何协调？</p>
<p> （4）整个程序运行过程中的监控？容错？重试？</p>
<p> 分布式方案需要考虑很多问题，但是我们可以将分布式程序中的公共功能封装成框 架，让开发人员将精力集中于业务逻辑上。而mapreduce就是这样一个分布式程序的通用框架。</p>
<h2 id="2-MapReduce核心思想"><a href="#2-MapReduce核心思想" class="headerlink" title="2. MapReduce核心思想"></a>2. MapReduce核心思想</h2><p>以wordcount为例：</p>
<p>需求：统计文件中每一个单词出现的总次数（查询结果a-p一个文件, q-z一个文件）</p>
<p><a href="20220710-1.jpg"><img src="/../../../../images/20220710-1.jpg" alt="mapreduce流程.jpg"></a></p>
<ul>
<li><p>总述：</p>
<p>1）分布式的运算程序往往需要分成至少2个阶段</p>
<p>2）第一个阶段的maptask并发实例，完全并行运行，互不相干</p>
<p>3）第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出</p>
<p>4）MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行</p>
</li>
<li><p>若干细节问题：</p>
<ol>
<li>Map task 如何进行任务分配</li>
<li>Reduce task 如何进行任务分配</li>
<li>Map task 和 Reduce task 如何衔接</li>
<li>Map task 如何都要自己负责数据的分区，很麻烦</li>
</ol>
</li>
</ul>
<h2 id="3-MapReduce-进程"><a href="#3-MapReduce-进程" class="headerlink" title="3. MapReduce 进程"></a>3. MapReduce 进程</h2><p>一个完整的mapreduce程序在分布式运行时有三类实例进程：</p>
<ol>
<li>MrAppMaster: 负责整个程序的过程调度以及状态协调。</li>
<li>MapTask: 负责map阶段的整个数据处理流程。</li>
<li>ReduceTask：负责Reduce阶段的整个数据处理流程。</li>
</ol>
<h2 id="4-MapReduce-编程规范"><a href="#4-MapReduce-编程规范" class="headerlink" title="4. MapReduce 编程规范"></a>4. MapReduce 编程规范</h2><p>户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端)</p>
<p>1）Mapper阶段</p>
<p> （1）用户自定义的Mapper要继承自己的父类</p>
<p> （2）Mapper的输入数据是KV对的形式（KV的类型可自定义）</p>
<p> （3）Mapper中的业务逻辑写在map()方法中</p>
<p> （4）Mapper的输出数据是KV对的形式（KV的类型可自定义）</p>
<p> （5）<strong>map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次</strong></p>
<p>2）Reducer阶段</p>
<p> （1）用户自定义的Reducer要继承自己的父类</p>
<p> （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</p>
<p> （3）Reducer的业务逻辑写在reduce()方法中</p>
<p> （4）<strong>Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法</strong></p>
<p>3）Driver阶段</p>
<p> 整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象</p>
<h2 id="5-MapReduce程序运行流程分析"><a href="#5-MapReduce程序运行流程分析" class="headerlink" title="5. MapReduce程序运行流程分析"></a>5. MapReduce程序运行流程分析</h2><p> 1）在MapReduce程序读取文件的输入目录上存放相应的文件。</p>
<p> 2）客户端程序在submit()方法执行前，获取待处理的数据信息，然后根据集群中参数的配置形成一个任务分配规划。</p>
<p> 3）客户端提交job.split、jar包、job.xml等文件给yarn，yarn中resourcemanager启动MRAppMaster。</p>
<p> 4）MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程。</p>
<p> 5）maptask利用客户指定的inputformat来读取数据，形成输入KV对。</p>
<p> 6）maptask将输入KV对传递给客户定义的map()方法，做逻辑运算</p>
<p> 7）<strong>map()运算完毕后将KV对收集到maptask缓存</strong>。</p>
<p> 8）<strong>maptask缓存中的KV对 按照K分区排序 后不断写到磁盘文件</strong></p>
<p> 9）MRAppMaster监控到所有maptask进程任务完成之后，<strong>会根据客户指定的参数启动相应数量的reducetask进程</strong>，并告知reducetask进程要处理的数据分区。</p>
<p> 10）Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，<strong>从若干台maptask运行所在机器上获取到若干个maptask输出结果文件</strong>，并在本地进行<strong>重新归并排序</strong>，然后按照<strong>相同key的KV为一个组</strong>，<strong>调用客户定义的reduce()方法</strong>进行逻辑运算。</p>
<p> 11）Reducetask运算完毕后，调用客户指定的outputformat将结果数据输出到外部存储。</p>
<h1 id="框架原理"><a href="#框架原理" class="headerlink" title="框架原理"></a>框架原理</h1><h2 id="1-MapReduce工作流程"><a href="#1-MapReduce工作流程" class="headerlink" title="1. MapReduce工作流程"></a>1. MapReduce工作流程</h2><ul>
<li><p><strong>流程步骤</strong></p>
<ol>
<li><p>待处理文本</p>
</li>
<li><p>客户端程序在submit()方法执行前，获取待处理的数据信息，然后根据集群中参数的配置形成一个任务分配规划。</p>
<p>如： test1.txt 0-128</p>
<p> test1.txt 128-240</p>
<p> test2.txt</p>
</li>
<li><p>提交切片信息（客户端提交job.split、jar包、job.xml等文件给yarn）</p>
</li>
<li><p>计算出maptask的数量</p>
</li>
<li><p>根据inputformat来读取数据</p>
</li>
<li><p>map(),进行逻辑运算</p>
</li>
<li><p>运算完毕后将KV对收集到maptask缓存（默认100M）</p>
</li>
<li><p>根据key分区，排序</p>
</li>
<li><p>缓冲区溢出，写入文件（分区且区内有序）</p>
</li>
<li><p>归并排序，将溢出的文件合并成大文件</p>
</li>
<li><p>所有map任务完成后，启动相应数量的reducetask, 并告知reducetask处理数据的范围（数据分区）</p>
</li>
<li><p>reducetask下载待处理数据到本地磁盘</p>
</li>
<li><p>归并排序，合并文件</p>
</li>
<li><p>一次读取一组</p>
</li>
<li><p>分组（groupingComparator）</p>
</li>
<li><p>outputformat将结果数据输出到外部存储。</p>
</li>
</ol>
</li>
<li><p><strong>shuffle流程详解 (7-16步)</strong></p>
<p>1）maptask收集我们的map()方法输出的kv对，放到内存缓冲区中</p>
<p>2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p>
<p>3）多个溢出文件会被合并成大的溢出文件</p>
<p><strong>4）在溢出过程中，及合并的过程中，都要调用partitioner进行分区和针对key进行排序</strong></p>
<p><strong>5）reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据</strong></p>
<p><strong>6）reducetask会取到同一个分区的来自不同maptask的结果文件， reducetask会将这些文件再进行合并（归并排序）</strong></p>
<p><strong>7）合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法）</strong></p>
</li>
<li><p><strong>注意</strong></p>
<p>Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，<strong>缓冲区越大，磁盘io的次数越少，执行速度就越快。</strong></p>
<p><strong>缓冲区的大小可以通过参数调整，参数：io.sort.mb 默认100M</strong></p>
</li>
</ul>
<h2 id="2-Writable序列化"><a href="#2-Writable序列化" class="headerlink" title="2. Writable序列化"></a>2. Writable序列化</h2><p> 序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。</p>
<p> 反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。</p>
<p> <strong>Java的序列化是一个重量级序列化框架（Serializable）</strong>，一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），<strong>不便于在网络中高效传输</strong>。所以，<strong>hadoop自己开发了一套序列化机制（Writable），精简、高效。</strong></p>
<h3 id="2-1-常用数据序列化类型"><a href="#2-1-常用数据序列化类型" class="headerlink" title="2.1 常用数据序列化类型"></a>2.1 常用数据序列化类型</h3><p>常用的数据类型对应的hadoop数据序列化类型</p>
<p><a href="20220710-2.jpg"><img src="/../../../../images/20220710-2.jpg" alt="writable.jpg"></a></p>
<p>###　2.2 自定义bean对象实现序列化接口</p>
<ul>
<li><p>自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项。</p>
<p><strong>（1）必须实现Writable接口</strong></p>
<p><strong>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造</strong></p>
<p><strong>（3）重写序列化方法</strong></p>
<p><strong>（4）重写反序列化方法</strong></p>
<p><strong>（5）注意反序列化的顺序和序列化的顺序完全一致</strong></p>
<p><strong>（6）要想把结果显示在文件中，需要重写toString()，且用”\t”分开，方便后续用</strong></p>
<p><strong>（7）如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序</strong></p>
</li>
</ul>
<p>以统计每一个手机号耗费的总上行流量、下行流量、总流量（序列化），为例**</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">// 1 必须实现Writable接口</span><br><span class="line">public class FlowBean implements Writable &#123;	</span><br><span class="line">	private long upFlow;	</span><br><span class="line">	private long downFlow;	</span><br><span class="line">	private long sumFlow;	</span><br><span class="line">	</span><br><span class="line">	//2 反序列化时，需要反射调用空参构造函数，所以必须有	</span><br><span class="line">	public FlowBean() &#123;		</span><br><span class="line">		super();	</span><br><span class="line">	&#125;	</span><br><span class="line">	</span><br><span class="line">	/**	 </span><br><span class="line">	* 3重写序列化方法	 </span><br><span class="line">	* 	 </span><br><span class="line">	* @param out	 </span><br><span class="line">	* @throws IOException	 </span><br><span class="line">	*/	</span><br><span class="line">	@Override	</span><br><span class="line">	public void write(DataOutput out) throws IOException &#123;		</span><br><span class="line">		out.writeLong(upFlow);		</span><br><span class="line">		out.writeLong(downFlow);		</span><br><span class="line">		out.writeLong(sumFlow);	</span><br><span class="line">	&#125;	</span><br><span class="line">	</span><br><span class="line">	/**	 </span><br><span class="line">	* 4 重写反序列化方法 </span><br><span class="line">	* 5 注意反序列化的顺序和序列化的顺序完全一致	 </span><br><span class="line">	* 	 </span><br><span class="line">	* @param in	 </span><br><span class="line">	* @throws IOException	 </span><br><span class="line">	*/	</span><br><span class="line">	@Override	</span><br><span class="line">	public void readFields(DataInput in) throws IOException &#123;		</span><br><span class="line">		upFlow = in.readLong();		</span><br><span class="line">		downFlow = in.readLong();		</span><br><span class="line">		sumFlow = in.readLong();	</span><br><span class="line">	&#125;    </span><br><span class="line">	</span><br><span class="line">	// 6要想把结果显示在文件中，需要重写toString()，且用”\t”分开，方便后续用	</span><br><span class="line">	@Override	</span><br><span class="line">	public String toString() &#123;		</span><br><span class="line">		return upFlow + &quot;\t&quot; + downFlow + &quot;\t&quot; + sumFlow;	</span><br><span class="line">	&#125;    </span><br><span class="line">	</span><br><span class="line">	//7 如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，</span><br><span class="line">	//因为mapreduce框中的shuffle过程一定会对key进行排序	</span><br><span class="line">	@Override	</span><br><span class="line">	public int compareTo(FlowBean o) &#123;		</span><br><span class="line">		// 倒序排列，从大到小		</span><br><span class="line">		return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;	</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-InputFormat数据切片机制"><a href="#3-InputFormat数据切片机制" class="headerlink" title="3. InputFormat数据切片机制"></a>3. InputFormat数据切片机制</h2><h3 id="3-1-job提交流程源码详解"><a href="#3-1-job提交流程源码详解" class="headerlink" title="3.1 job提交流程源码详解"></a>3.1 job提交流程源码详解</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">1）job提交流程源码详解</span><br><span class="line"></span><br><span class="line">waitForCompletion()</span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line">// 1建立连接	</span><br><span class="line">	connect();			</span><br><span class="line">		// 1）创建提交job的代理		</span><br><span class="line">		new Cluster(getConfiguration());	</span><br><span class="line">        </span><br><span class="line">		// 2）判断是本地yarn还是远程		</span><br><span class="line">		initialize(jobTrackAddr, conf); </span><br><span class="line">		</span><br><span class="line">// 2 提交jobsubmitter.submitJobInternal(Job.this, cluster)	</span><br><span class="line">	// 1）创建给集群提交数据的Stag路径	</span><br><span class="line">	Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);	</span><br><span class="line">	</span><br><span class="line">	// 2）获取jobid ，并创建job路径	</span><br><span class="line">	JobID jobId = submitClient.getNewJobID();	</span><br><span class="line">	</span><br><span class="line">	// 3）拷贝jar包到集群</span><br><span class="line">	copyAndConfigureFiles(job, submitJobDir);		</span><br><span class="line">	rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">	</span><br><span class="line">	// 4）计算切片，生成切片规划文件</span><br><span class="line">	writeSplits(job, submitJobDir);	</span><br><span class="line">	maps = writeNewSplits(job, jobSubmitDir);		</span><br><span class="line">	input.getSplits(job);</span><br><span class="line">	</span><br><span class="line">	// 5）向Stag路径写xml配置文件</span><br><span class="line">	writeConf(conf, submitJobFile);	</span><br><span class="line">	conf.writeXml(out);</span><br><span class="line">	</span><br><span class="line">	// 6）提交job,返回提交状态</span><br><span class="line">	status = submitClient.submitJob(jobId, submitJobDir.toString(),job.getCredentials());</span><br></pre></td></tr></table></figure>

<h3 id="3-2-FileInputFormat源码解析-input-getSplits-job"><a href="#3-2-FileInputFormat源码解析-input-getSplits-job" class="headerlink" title="3.2 FileInputFormat源码解析(input.getSplits(job))"></a>3.2 FileInputFormat源码解析(input.getSplits(job))</h3><p><a href="20220710-3.jpg"><img src="/../../../../images/20220710-3.jpg" alt="FileInputFormat.jpg"></a></p>
<p>（1）找到你数据存储的目录。</p>
<p>（2）开始遍历处理（规划切片）目录下的每一个文件</p>
<p>（3）遍历第一个文件ss.txt</p>
<p> a）获取文件大小fs.sizeOf(ss.txt);</p>
<p> b）计算切片大小 computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M</p>
<p> c）默认情况下，切片大小&#x3D;blocksize</p>
<p> d）开始切，形成第1个切片：ss.txt—0:128M第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）</p>
<p> e）将切片信息写到一个切片规划文件中</p>
<p> f）整个切片的核心过程在getSplit()方法中完成。</p>
<p> g）数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。</p>
<p>h）注意：block是HDFS上物理上存储的存储的数据，切片是对数据逻辑上的划分。</p>
<p>（4）提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。</p>
<h3 id="3-3-FileInputFormat中默认的切片机制："><a href="#3-3-FileInputFormat中默认的切片机制：" class="headerlink" title="3.3 FileInputFormat中默认的切片机制："></a>3.3 FileInputFormat中默认的切片机制：</h3><p>（1）简单地按照文件的内容长度进行切片</p>
<p>（2）切片大小，默认等于block大小</p>
<p>（3）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</p>
<p>比如待处理数据有两个文件：</p>
<p> file1.txt 320M</p>
<p> file2.txt 10M</p>
<p>经过FileInputFormat的切片机制运算后，形成的切片信息如下：</p>
<p> file1.txt.split1– 0~128</p>
<p> file1.txt.split2– 128~256</p>
<p> file1.txt.split3– 256~320</p>
<p> file2.txt.split1– 0~10M</p>
<h3 id="3-4-FileInputFormat切片大小的参数配置"><a href="#3-4-FileInputFormat切片大小的参数配置" class="headerlink" title="3.4 FileInputFormat切片大小的参数配置"></a>3.4 FileInputFormat切片大小的参数配置</h3><p>（1）通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize,Math.min(maxSize, blockSize));</p>
<p>切片主要由这几个值来运算决定:</p>
<p>mapreduce.input.fileinputformat.split.minsize&#x3D;1默认值为1</p>
<p>mapreduce.input.fileinputformat.split.maxsize&#x3D;Long.MAXValue 默认值Long.MAXValue</p>
<p>因此，默认情况下，切片大小&#x3D;blocksize。</p>
<p> maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。</p>
<p> minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。</p>
<p>（2）获取切片信息API</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 根据文件类型获取切片信息</span><br><span class="line">FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">// 获取切片的文件名称</span><br><span class="line">String name = inputSplit.getPath().getName();</span><br></pre></td></tr></table></figure>

<h3 id="3-5-CombineTextInputFormat切片机制"><a href="#3-5-CombineTextInputFormat切片机制" class="headerlink" title="3.5 CombineTextInputFormat切片机制"></a>3.5 CombineTextInputFormat切片机制</h3><p>关于大量小文件的优化策略</p>
<p>1）默认情况下TextInputformat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。</p>
<p>2）优化策略</p>
<p> （1）最好的办法，在数据处理系统的最前端（预处理&#x2F;采集），将小文件先合并成大文件，再上传到HDFS做后续分析。</p>
<p> （2）补救措施：如果已经是大量小文件在HDFS中了，可以使用另一种InputFormat来做切片（CombineTextInputFormat），它的切片逻辑跟TextFileInputFormat不同：它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个maptask。</p>
<p> （3）优先满足最小切片大小，不超过最大切片大小</p>
<p> CombineTextInputFormat.<em>setMaxInputSplitSize</em>(job,4194304);&#x2F;&#x2F; 128m</p>
<p> CombineTextInputFormat.<em>setMinInputSplitSize</em>(job,2097152);&#x2F;&#x2F; 2m</p>
<p> 举例：0.5m+1m+0.3m+5m&#x3D;2m + 4.8m&#x3D;2m + 4m + 0.8m</p>
<p>3）具体实现步骤</p>
<p> &#x2F;&#x2F;如果不设置InputFormat,它默认用的是TextInputFormat.class</p>
<p> job.setInputFormatClass(CombineTextInputFormat.<strong>class</strong>)</p>
<p> CombineTextInputFormat.<em>setMaxInputSplitSize</em>(job,4194304);&#x2F;&#x2F; 4m</p>
<p> CombineTextInputFormat.<em>setMinInputSplitSize</em>(job,2097152);&#x2F;&#x2F; 2m</p>
<h3 id="3-6-自定义InputFormat"><a href="#3-6-自定义InputFormat" class="headerlink" title="3.6 自定义InputFormat"></a>3.6 自定义InputFormat</h3><p><strong>概述</strong></p>
<p>（1）自定义一个类继承FileInputFormat</p>
<p>（2）改写RecordReader，实现一次读取一个完整文件封装为KV</p>
<p>（3）在输出时使用SequenceFileOutPutFormat输出合并文件</p>
<h2 id="4-MapTask工作机制"><a href="#4-MapTask工作机制" class="headerlink" title="4. MapTask工作机制"></a>4. MapTask工作机制</h2><h2 id="5-Shuffle机制"><a href="#5-Shuffle机制" class="headerlink" title="5. Shuffle机制"></a>5. Shuffle机制</h2><h2 id="6-ReduceTask工作机制"><a href="#6-ReduceTask工作机制" class="headerlink" title="6. ReduceTask工作机制"></a>6. ReduceTask工作机制</h2><h2 id="7-自定义OutputFormat"><a href="#7-自定义OutputFormat" class="headerlink" title="7. 自定义OutputFormat"></a>7. 自定义OutputFormat</h2><h2 id="8-计数器应用"><a href="#8-计数器应用" class="headerlink" title="8. 计数器应用"></a>8. 计数器应用</h2><h2 id="9-数据清洗"><a href="#9-数据清洗" class="headerlink" title="9. 数据清洗"></a>9. 数据清洗</h2><h1 id="MapReduce与Yarn"><a href="#MapReduce与Yarn" class="headerlink" title="MapReduce与Yarn"></a>MapReduce与Yarn</h1>
    </div>

    <div></div>
  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce%E6%A6%82%E8%BF%B0"><span class="toc-text">MapReduce概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-MapReduce%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%B9%E6%A1%88%E8%80%83%E8%99%91%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">1. MapReduce分布式方案考虑的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-MapReduce%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">2. MapReduce核心思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-MapReduce-%E8%BF%9B%E7%A8%8B"><span class="toc-text">3. MapReduce 进程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-MapReduce-%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83"><span class="toc-text">4. MapReduce 编程规范</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-MapReduce%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90"><span class="toc-text">5. MapReduce程序运行流程分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86"><span class="toc-text">框架原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-text">1. MapReduce工作流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Writable%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">2. Writable序列化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96%E7%B1%BB%E5%9E%8B"><span class="toc-text">2.1 常用数据序列化类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-InputFormat%E6%95%B0%E6%8D%AE%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-text">3. InputFormat数据切片机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-job%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3"><span class="toc-text">3.1 job提交流程源码详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-FileInputFormat%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-input-getSplits-job"><span class="toc-text">3.2 FileInputFormat源码解析(input.getSplits(job))</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-FileInputFormat%E4%B8%AD%E9%BB%98%E8%AE%A4%E7%9A%84%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6%EF%BC%9A"><span class="toc-text">3.3 FileInputFormat中默认的切片机制：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-FileInputFormat%E5%88%87%E7%89%87%E5%A4%A7%E5%B0%8F%E7%9A%84%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="toc-text">3.4 FileInputFormat切片大小的参数配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-CombineTextInputFormat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-text">3.5 CombineTextInputFormat切片机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E8%87%AA%E5%AE%9A%E4%B9%89InputFormat"><span class="toc-text">3.6 自定义InputFormat</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-MapTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-text">4. MapTask工作机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Shuffle%E6%9C%BA%E5%88%B6"><span class="toc-text">5. Shuffle机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-ReduceTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-text">6. ReduceTask工作机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E8%87%AA%E5%AE%9A%E4%B9%89OutputFormat"><span class="toc-text">7. 自定义OutputFormat</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E8%AE%A1%E6%95%B0%E5%99%A8%E5%BA%94%E7%94%A8"><span class="toc-text">8. 计数器应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97"><span class="toc-text">9. 数据清洗</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce%E4%B8%8EYarn"><span class="toc-text">MapReduce与Yarn</span></a></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by/4.0/">知识共享署名 4.0 国际许可协议</a>
    <span>进行许可。 转载时请注明原文链接。</span>
</div>
<div class="share" style="width: 100%;">
  <img src="/../../../../images/wechat.png" alt="Running Geek" style="margin: auto; display: block;"/>

  <div style="margin: auto; text-align: center; font-size: 0.8em; color: grey;">微信扫一扫向我投食</div>
  
</div>

  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/2019/03/04/%E6%AF%95%E4%B8%9A%E5%AD%A3%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93/" rel="next" title="毕业季刷题总结">
          毕业季刷题总结
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/2019/04/22/Hive-SQL/" rel="prev" title="HiveSQL">
            HiveSQL
          </a>
          <span>〉</span>
        
      </div>
    </div>
  


    </div>

    

  </div>
  <footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" target="_blank" rel="noopener" href="https://zshlovely.github.io/">首页</a> |
        <a class="bottom-item" href="https://zshlovely.github.io/" target="_blank">主站</a> |
        <a class="bottom-item" href="https://github.com/fenghuayangyi" target="_blank">GitHub</a>
    </div>
</footer>
  

<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>

  



</body>
</html>
