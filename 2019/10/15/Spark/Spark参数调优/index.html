
<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
  
  
    <meta name="keywords" content="Spark," />
  

  
    <meta name="description" content="既然选择远方，便只顾风雨兼程" />
  
  
  <link rel="icon" type="image/x-icon" href="/logo.png">
  <title>Spark参数调优 [ shaohua&#39;s blog ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  <div class="nav-container">
    <nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    <img class="avatar" src="/images/logo.png">
    <span class="title">shaohua&#39;s blog</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">归档</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/tags" class="pure-menu-link">标签</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/categories" class="pure-menu-link">分类</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
          
      
  </ul>
   
</nav>
  </div>

  <div class="container" id="content-outer">
    <div class="inner" id="content-inner">
      <div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        Spark参数调优
      </h1>
      <span>
        
        <time class="time" datetime="2019-10-14T17:10:20.000Z">
        2019-10-15
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">
      <span id="busuanzi_value_page_pv"></span> 点击
    </span>
      <span class="slash">/</span>
      <span class="read">阅读耗时 22 分钟</span>
    </header>

    <div class="post-content">
      <h1 id="Spark参数调优"><a href="#Spark参数调优" class="headerlink" title="Spark参数调优"></a>Spark参数调优</h1><p>参考文献：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zyzzxycj/article/details/81011540">https://blog.csdn.net/zyzzxycj/article/details/81011540</a></p>
<p>因为每个集群规模都不一样，只有理解了参数的用途，调试出符合自己业务场景集群环境，并且能在扩大集群、业务的情况下，能够跟着修改参数才算是正确的参数调优。我的目标就是能够熟练的进行参数调优。</p>
<h2 id="1-Application-Properties-应用基本属性"><a href="#1-Application-Properties-应用基本属性" class="headerlink" title="1*. Application Properties 应用基本属性"></a>1*. Application Properties 应用基本属性</h2><p>这些参数比较重要，执行spark的shell 一般都需要配置一下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">spark.driver.cores  	</span><br><span class="line">	driver端分配的核数，默认为1,资源充足的话可以尽量给多。</span><br><span class="line">	</span><br><span class="line">spark.driver.memory	</span><br><span class="line">	driver端分配的内存数，默认为1g，资源充足的话可以尽量给多。</span><br><span class="line">	</span><br><span class="line">spark.executor.memory	</span><br><span class="line">	每个executor分配的内存数，默认1g，会受到yarn CDH的限制，</span><br><span class="line">	和memoryOverhead相加 不能超过总内存限制。</span><br><span class="line"></span><br><span class="line">spark.driver.maxResultSize	</span><br><span class="line">	driver端接收的最大结果大小，默认1GB，最小1MB，设置0为无限。	</span><br><span class="line">	这个参数不建议设置的太大,如果要做数据可视化,更应该控制在20-30MB以内,</span><br><span class="line">	过大会导致OOM(out of memory)。</span><br><span class="line">	</span><br><span class="line">spark.extraListeners.(不常用)	</span><br><span class="line">	默认none，随着SparkContext被创建而创建，用于监听单参数、无参数构造函数的创建，并抛出异常。</span><br><span class="line">	可用于自定义监听，实现监控spark的任务进度。</span><br></pre></td></tr></table></figure>

<h2 id="2-Runtime-Environment-运行环境"><a href="#2-Runtime-Environment-运行环境" class="headerlink" title="2. Runtime Environment 运行环境"></a>2. <strong>Runtime Environment 运行环境</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">主要是一些日志，jvm参数的额外配置，jars等一些自定义的配置(待补充)</span><br></pre></td></tr></table></figure>

<h2 id="3-Shuffle-Behavior"><a href="#3-Shuffle-Behavior" class="headerlink" title="3. Shuffle Behavior"></a>3. Shuffle Behavior</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">spark.reducer.maxSizeInFlight	</span><br><span class="line">	默认48m。从每个reduce任务同时拉取的最大map数，</span><br><span class="line">	每个reduce都会在完成任务后，需要一个堆外内存的缓冲区来存放结果，</span><br><span class="line">	如果没有充裕的内存就尽可能把这个调小一点。</span><br><span class="line">	相反，堆外内存充裕，调大些就能节省gc时间。</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">spark.reducer.maxBlocksInFlightPerAddress(一般不会改)	</span><br><span class="line">	限制了每个主机每次reduce可以被多少台远程主机拉取文件块，</span><br><span class="line">	调低这个参数可以有效减轻node manager的负载。（默认值Int.MaxValue）</span><br><span class="line"></span><br><span class="line">spark.reducer.maxReqsInFlight	</span><br><span class="line">	限制远程机器拉取本机器文件块的请求数，随着集群增大，需要对此做出限制。</span><br><span class="line">	否则可能会使本机负载过大而挂掉。（默认值为Int.MaxValue）</span><br><span class="line">	</span><br><span class="line">spark.reducer.maxReqSizeShuffleToMem	</span><br><span class="line">	shuffle请求的文件块大小 超过这个参数值，就会被强行落盘，</span><br><span class="line">	防止一大堆并发请求把内存占满。（默认Long.MaxValue）</span><br><span class="line">	</span><br><span class="line">spark.shuffle.compress	</span><br><span class="line">	是否压缩map输出文件，默认压缩 true</span><br><span class="line">	</span><br><span class="line">spark.shuffle.spill.compress	</span><br><span class="line">	shuffle过程中溢出的文件是否压缩，默认true，使用spark.io.compression.codec压缩。</span><br><span class="line">	</span><br><span class="line">spark.shuffle.file.buffer	</span><br><span class="line">	在内存输出流中 每个shuffle文件占用内存大小，适当提高 可以减少磁盘读写 io次数，初始值为32k。</span><br><span class="line">	</span><br><span class="line">spark.shuffle.memoryFraction	</span><br><span class="line">	该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。	</span><br><span class="line">	cache少且内存充足时，可以调大该参数，给shuffle read的聚合操作更多内存，</span><br><span class="line">	以避免由于内存不足导致聚合过程中频繁读写磁盘。</span><br><span class="line">	</span><br><span class="line">spark.shuffle.manager	</span><br><span class="line">	当ShuffleManager为SortShuffleManager时，</span><br><span class="line">	如果shuffle read task的数量小于这个阈值（默认是200），	 </span><br><span class="line">	则shuffle write过程中不会进行排序操作，</span><br><span class="line">	而是直接按照未经优化的HashShuffleManager的方式去写数据，</span><br><span class="line">	但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。	</span><br><span class="line">	当使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，</span><br><span class="line">	大于shuffle read 	task的数量。那么此时就会自动启用bypass机制，</span><br><span class="line">	map-side就不会进行排序了，减少了排序的性能开销。</span><br><span class="line">	但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">spark.shuffle.consolidateFiles	</span><br><span class="line">	如果使用HashShuffleManager，该参数有效。</span><br><span class="line">	如果设置为true，那么就会开启consolidate机制，会大幅度合并	 </span><br><span class="line">	shuffle write的输出文件，对于shuffle read task数量特别多的情况下，</span><br><span class="line">	这种方法可以极大地减少磁盘IO开销，提升性能。</span><br><span class="line">	</span><br><span class="line">spark.shuffle.io.maxRetries（重试次数）	</span><br><span class="line">	shuffle read task从shuffle write task所在节点拉取属于自己的数据时，</span><br><span class="line">	如果因为网络异常导致拉取失败，是会自动进行重试的。</span><br><span class="line">	该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，</span><br><span class="line">	就可能会导致作业执行失败。	</span><br><span class="line">	</span><br><span class="line">spark.shuffle.io.retryWait	</span><br><span class="line">	同上，默认5s，建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.io.encryption.enabled</span><br><span class="line">spark.io.encryption.keySizeBits</span><br><span class="line"></span><br><span class="line">spark.io.encryption.keygen.algorithm	</span><br><span class="line">	io加密，默认关闭</span><br></pre></td></tr></table></figure>

<h2 id="4-Spark-UI"><a href="#4-Spark-UI" class="headerlink" title="4. Spark UI"></a>4. Spark UI</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这一块配置，是有关于spark日志的。日志开关，日志输出路径，是否压缩。(待补充)</span><br></pre></td></tr></table></figure>

<h2 id="5-Compression-and-Serialization压缩与序列化"><a href="#5-Compression-and-Serialization压缩与序列化" class="headerlink" title="5. Compression and Serialization压缩与序列化"></a>5. Compression and Serialization压缩与序列化</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">spark.broadcast.compress	</span><br><span class="line">	广播变量前是否会先进行压缩。默认true （spark.io.compression.codec）</span><br><span class="line">	</span><br><span class="line">spark.io.compression.codec	</span><br><span class="line">	压缩RDD数据、日志、shuffle输出等的压缩格式 默认lz4</span><br><span class="line">	</span><br><span class="line">spark.io.compression.lz4.blockSize	</span><br><span class="line">	使用lz4压缩时，每个数据块大小 默认32k</span><br><span class="line">	</span><br><span class="line">spark.rdd.compress	</span><br><span class="line">	rdd是否压缩 默认false，节省memory_cache大量内存 消耗更多的cpu资源（时间）。</span><br><span class="line">	</span><br><span class="line">spark.serializer.objectStreamReset	</span><br><span class="line">	当使用JavaSerializer序列化时，会缓存对象防止写多余的数据，但这些对象就不会被gc，</span><br><span class="line">	可以输入reset 清空缓存。默认缓存100个对象，修改成-1则不缓存任何对象。</span><br></pre></td></tr></table></figure>

<p>压缩以后节省内存资源，消耗cpu资源。</p>
<h2 id="6-Memory-Management-内存管理"><a href="#6-Memory-Management-内存管理" class="headerlink" title="6. Memory Management 内存管理"></a>6. <strong>Memory Management 内存管理</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">spark.memory.fraction	</span><br><span class="line">	执行内存和缓存内存（堆）占jvm总内存的比例，</span><br><span class="line">	剩余的部分是spark留给用户存储内部源数据、数据结构、异常大的结果数据。</span><br><span class="line">	默认值0.6，调小会导致频繁gc，调大容易造成oom。</span><br><span class="line">	</span><br><span class="line">spark.memory.storageFraction	</span><br><span class="line">	用于存储的内存在堆中的占比，默认0.5。</span><br><span class="line">	调大会导致执行内存过小，执行数据落盘，影响效率；调小会导致缓存内存不够，缓存到磁盘上去，影响效率。</span><br><span class="line">	另外，执行内存和缓存内存公用java堆，当执行内存没有使用时，会动态分配给缓存内存使用，</span><br><span class="line">	反之也是这样。</span><br><span class="line">	如果执行内存不够用，可以将存储内存释放移动到磁盘上（最多释放不能超过本参数划分的比例），</span><br><span class="line">	但存储内存不能把执行内存抢走。</span><br><span class="line">	</span><br><span class="line">spark.memory.offHeap.enabled	</span><br><span class="line">	是否允许使用堆外内存来进行某些操作。默认false</span><br><span class="line">	</span><br><span class="line">spark.memory.offHeap.size	</span><br><span class="line">	允许使用进行操作的堆外内存的大小，单位bytes 默认0</span><br><span class="line">	</span><br><span class="line">spark.storage.replication.proactive	</span><br><span class="line">	针对失败的executor，主动去cache 有关的RDD中的数据。默false</span><br><span class="line">	</span><br><span class="line">spark.cleaner.periodicGC.interval	</span><br><span class="line">	控制触发gc的频率，默认30min</span><br><span class="line">	</span><br><span class="line">spark.cleaner.referenceTracking	</span><br><span class="line">	是否进行context cleaning，默认true</span><br><span class="line">	</span><br><span class="line">spark.cleaner.referenceTracking.blocking	</span><br><span class="line">	清理线程是否应该阻止清理任务，默认true</span><br><span class="line">	</span><br><span class="line">spark.cleaner.referenceTracking.blocking.shuffle	</span><br><span class="line">	清理线程是否应该阻止shuffle的清理任务，默false</span><br><span class="line">	</span><br><span class="line">spark.cleaner.referenceTracking.cleanCheckpoints	</span><br><span class="line">	清理线程是否应该清理依赖超出范围的检查点文件，默认false</span><br></pre></td></tr></table></figure>

<h2 id="7-Executor-behavior"><a href="#7-Executor-behavior" class="headerlink" title="7. Executor behavior"></a>7. Executor behavior</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">spark.broadcast.blockSize	</span><br><span class="line">	TorrentBroadcastFactory中的每一个block大小，默认4m	</span><br><span class="line">	过大会减少广播时的并行度，过小会导致BlockManager 产生 performance hit.</span><br><span class="line">	（暂时没懂这是干啥用的）</span><br><span class="line">	</span><br><span class="line">spark.executor.cores	</span><br><span class="line">	每个executor的核数，默认yarn下1核，standalone下为所有可用的核。</span><br><span class="line">	</span><br><span class="line">spark.default.parallelism	</span><br><span class="line">	默认RDD的分区数、并行数。	</span><br><span class="line">	像reduceByKey和join等这种需要分布式shuffle的操作中，最大父RDD的分区数；	</span><br><span class="line">	像parallelize之类没有父RDD的操作，则取决于运行环境下得cluster manager：		</span><br><span class="line">	如果为单机模式，本机核数；		</span><br><span class="line">	集群模式为所有executor总核数与2 中最大的一个。</span><br><span class="line">	</span><br><span class="line">spark.executor.heartbeatInterval	</span><br><span class="line">	executor和driver心跳发送间隔，默认10s，</span><br><span class="line">	必须远远小于spark.network.timeoutspark.files.fetchTimeout	</span><br><span class="line">	从driver端执行SparkContext.addFile() 抓取添加的文件的超时时间，</span><br><span class="line">	默认60s</span><br><span class="line">	</span><br><span class="line">spark.files.useFetchCache	</span><br><span class="line">	默认true，如果设为true，拉取文件时会在同一个application中本地持久化，</span><br><span class="line">	被若干个executors共享。这使得当同一个主机下有多个executors时，执行任务效率提高。</span><br><span class="line">	</span><br><span class="line">spark.files.overwrite	</span><br><span class="line">	默认false，是否在执行SparkContext.addFile() 添加文件时，覆盖已有的内容有差异的文件。</span><br><span class="line">	</span><br><span class="line">spark.files.maxPartitionBytes	</span><br><span class="line">	单partition中最多能容纳的文件大小，单位Bytes </span><br><span class="line">	默认134217728 (128 MB)</span><br><span class="line">	</span><br><span class="line">spark.files.openCostInBytes	</span><br><span class="line">	小文件合并阈值，小于该参数就会被合并到一个partition内。	</span><br><span class="line">	默认4194304 (4 MB) 。</span><br><span class="line">	这个参数在将多个文件放入一个partition时被用到，宁可设置的小一些，</span><br><span class="line">	因为在partition操作中，小文件肯定会比大文件快。</span><br><span class="line">	</span><br><span class="line">spark.storage.memoryMapThreshold	</span><br><span class="line">	从磁盘上读文件时，最小单位不能少于该设定值，默认2m，小于或者接近操作系统的每个page的大小。</span><br></pre></td></tr></table></figure>

<h2 id="8-Networking网络"><a href="#8-Networking网络" class="headerlink" title="8. Networking网络"></a>8. Networking网络</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">网络超时或者ip端口的一些配置</span><br></pre></td></tr></table></figure>

<h2 id="9-Scheduling调度"><a href="#9-Scheduling调度" class="headerlink" title="9. Scheduling调度"></a>9. Scheduling调度</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">spark.scheduler.maxRegisteredResourcesWaitingTime	</span><br><span class="line">	在执行前最大等待申请资源的时间，默认30s。</span><br><span class="line">	</span><br><span class="line">spark.scheduler.minRegisteredResourcesRatio	</span><br><span class="line">	实际注册的资源数占预期需要的资源数的比例，默认0.8</span><br><span class="line">	</span><br><span class="line">spark.scheduler.mode</span><br><span class="line">	其他进阶的配置如下：		</span><br><span class="line">	调度模式，默认FIFO 先进队列先调度，可以选择FAIR。</span><br><span class="line">	</span><br><span class="line">spark.scheduler.revive.interval	</span><br><span class="line">	work回复重启的时间间隔，默认1s</span><br><span class="line">	</span><br><span class="line">spark.scheduler.listenerbus.eventqueue.capacity	</span><br><span class="line">	spark事件监听队列容量，默认10000，必须为正值，增加可能会消耗更多内存</span><br><span class="line">	spark.blacklist.enabled***************************************************************	</span><br><span class="line">	是否列入黑名单，默认false。如果设成true，当一个executor失败好几次时，会被列入黑名单，防止后续task		派发到这个executor。可以进一步调节spark.blacklist以下相关的参数：	</span><br><span class="line">	（均为测试参数 Experimental）	</span><br><span class="line">	spark.blacklist.timeout    </span><br><span class="line">	spark.blacklist.task.maxTaskAttemptsPerExecutor    	</span><br><span class="line">	spark.blacklist.task.maxTaskAttemptsPerNode    </span><br><span class="line">	spark.blacklist.stage.maxFailedTasksPerExecutor    </span><br><span class="line">	spark.blacklist.application.maxFailedExecutorsPerNode    </span><br><span class="line">	spark.blacklist.killBlacklistedExecutors</span><br><span class="line">	spark.blacklist.application.fetchFailure.enabled</span><br><span class="line"></span><br><span class="line">spark.speculation************************************************************************	推测，如果有task执行的慢了，就会重新执行它。默认false.	</span><br><span class="line"></span><br><span class="line">	详细相关配置如下：	</span><br><span class="line">	spark.speculation.interval		检查task快慢的频率，推测间隔，默认100ms。	</span><br><span class="line">	spark.speculation.multiplier		推测比均值慢几次算是task执行过慢，默认1.5.	</span><br><span class="line">	spark.speculation.quantile		在某个stage，完成度必须达到该参数的比例，才能被推测，默认0.75</span><br><span class="line">	spark.task.cpus	每个task分配的cpu数，默认1</span><br><span class="line">	spark.task.maxFailures	在放弃这个job前允许的最大失败次数，默认4</span><br><span class="line">	spark.task.reaper.enabled</span><br><span class="line">		(原先有 job失败了但一直显示有task在running，总算找到这个参数了)********	</span><br><span class="line">	赋予spark监控有权限去kill那些失效的task，默认false</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">其他进阶的配置如下：	</span><br><span class="line">	spark.task.reaper.pollingInterval		</span><br><span class="line">	轮询被kill掉的task的时间间隔，如果还在running，就会打warn日志，默认10s。	</span><br><span class="line">	</span><br><span class="line">	spark.task.reaper.threadDump		</span><br><span class="line">	线程回收是是否产生日志，默认true。	</span><br><span class="line">	</span><br><span class="line">	spark.task.reaper.killTimeout		</span><br><span class="line">	当一个被kill的task过了多久还在running，就会把那个executor给kill掉，默认-1。	</span><br><span class="line">	</span><br><span class="line">	spark.stage.maxConsecutiveAttempts		</span><br><span class="line">	在终止前，一个stage连续尝试次数，默认4。</span><br></pre></td></tr></table></figure>

<h2 id="10-Dynamic-Allocation动态分配"><a href="#10-Dynamic-Allocation动态分配" class="headerlink" title="10. Dynamic Allocation动态分配"></a>10. Dynamic Allocation动态分配</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">spark.dynamicAllocation.enabled(这个配置只能动态调整executor的数量)	</span><br><span class="line">	是否开启动态资源配置，根据工作负载来衡量是否应该增加或减少executor，默认false</span><br><span class="line">	</span><br><span class="line">	以下相关参数：	</span><br><span class="line">	spark.dynamicAllocation.minExecutors		</span><br><span class="line">	动态分配最小executor个数，在启动时就申请好的，默认0	</span><br><span class="line">	</span><br><span class="line">	spark.dynamicAllocation.maxExecutors		</span><br><span class="line">	动态分配最大executor个数，默认infinity	</span><br><span class="line">	</span><br><span class="line">	spark.dynamicAllocation.initialExecutors</span><br><span class="line">	</span><br><span class="line">	动态分配初始executor个数默认值为</span><br><span class="line">	spark.dynamicAllocation.minExecutors</span><br><span class="line">	</span><br><span class="line">	spark.dynamicAllocation.executorIdleTimeout	</span><br><span class="line">	当某个executor空闲超过这个设定值，就会被kill，默认60s</span><br><span class="line">	</span><br><span class="line">	spark.dynamicAllocation.cachedExecutorIdleTimeout	</span><br><span class="line">	当某个缓存数据的executor空闲时间超过这个设定值，就会被kill，默认infinity</span><br><span class="line">	</span><br><span class="line">	spark.dynamicAllocation.schedulerBacklogTimeout	</span><br><span class="line">	任务队列非空，资源不够，申请executor的时间间隔，默认1s</span><br><span class="line">	</span><br><span class="line">	spark.dynamicAllocation.sustainedSchedulerBacklogTimeout	</span><br><span class="line">	同schedulerBacklogTimeout，是申请了新executor之后继续申请的间隔，</span><br><span class="line">	默认=schedulerBacklogTimeout</span><br></pre></td></tr></table></figure>

<h2 id="11-Spark-Streaming"><a href="#11-Spark-Streaming" class="headerlink" title="11. Spark Streaming"></a>11. <strong>Spark Streaming</strong></h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zyzzxycj/article/details/82428031">https://blog.csdn.net/zyzzxycj/article/details/82428031</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Spark Streaming</span><br></pre></td></tr></table></figure>
    </div>

    <div></div>
  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="toc-text">Spark参数调优</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Application-Properties-%E5%BA%94%E7%94%A8%E5%9F%BA%E6%9C%AC%E5%B1%9E%E6%80%A7"><span class="toc-text">1*. Application Properties 应用基本属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Runtime-Environment-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83"><span class="toc-text">2. Runtime Environment 运行环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Shuffle-Behavior"><span class="toc-text">3. Shuffle Behavior</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Spark-UI"><span class="toc-text">4. Spark UI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Compression-and-Serialization%E5%8E%8B%E7%BC%A9%E4%B8%8E%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">5. Compression and Serialization压缩与序列化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Memory-Management-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">6. Memory Management 内存管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Executor-behavior"><span class="toc-text">7. Executor behavior</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Networking%E7%BD%91%E7%BB%9C"><span class="toc-text">8. Networking网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Scheduling%E8%B0%83%E5%BA%A6"><span class="toc-text">9. Scheduling调度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-Dynamic-Allocation%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D"><span class="toc-text">10. Dynamic Allocation动态分配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-Spark-Streaming"><span class="toc-text">11. Spark Streaming</span></a></li></ol></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by/4.0/">知识共享署名 4.0 国际许可协议</a>
    <span>进行许可。 转载时请注明原文链接。</span>
</div>
<div class="share" style="width: 100%;">
  <img src="/../../../../images/wechat.png" alt="Running Geek" style="margin: auto; display: block;"/>

  <div style="margin: auto; text-align: center; font-size: 0.8em; color: grey;">微信扫一扫向我投食</div>
  
</div>

  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/2019/10/01/Hadoop/MapReduce%E7%AC%94%E8%AE%B0/" rel="next" title="MapReduce笔记">
          MapReduce笔记
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/2019/10/16/Scala%E7%AC%94%E8%AE%B0/" rel="prev" title="Scala笔记">
            Scala笔记
          </a>
          <span>〉</span>
        
      </div>
    </div>
  


    </div>

    

  </div>
  <footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" target="_blank" rel="noopener" href="https://zshlovely.github.io/">首页</a> |
        <a class="bottom-item" href="https://zshlovely.github.io/" target="_blank">主站</a> |
        <a class="bottom-item" href="https://github.com/fenghuayangyi" target="_blank">GitHub</a>
    </div>
</footer>
  

<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>

  



</body>
</html>
