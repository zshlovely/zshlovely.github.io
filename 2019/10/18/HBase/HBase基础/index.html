
<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
  
  
    <meta name="keywords" content="HBase," />
  

  
    <meta name="description" content="既然选择远方，便只顾风雨兼程" />
  
  
  <link rel="icon" type="image/x-icon" href="/logo.png">
  <title>HBase基础 [ shaohua&#39;s blog ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  <div class="nav-container">
    <nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    <img class="avatar" src="/images/logo.png">
    <span class="title">shaohua&#39;s blog</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">归档</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/tags" class="pure-menu-link">标签</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/categories" class="pure-menu-link">分类</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
          
      
  </ul>
   
</nav>
  </div>

  <div class="container" id="content-outer">
    <div class="inner" id="content-inner">
      <div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        HBase基础
      </h1>
      <span>
        
        <time class="time" datetime="2019-10-17T17:10:20.000Z">
        2019-10-18
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">
      <span id="busuanzi_value_page_pv"></span> 点击
    </span>
      <span class="slash">/</span>
      <span class="read">阅读耗时 40 分钟</span>
    </header>

    <div class="post-content">
      <h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><p>或者说 HBase是在Hadoop和ZooKeeper之上构建<strong>非关系型</strong>，<strong>面向列存储</strong>的开源分布式结构化数据存储系统。</p>
<p>HBase的部署前提：Zookeeper + Hadoop</p>
<h2 id="1-HBase-介绍"><a href="#1-HBase-介绍" class="headerlink" title="1. HBase 介绍"></a>1. HBase 介绍</h2><ul>
<li><p>HBase和Hive</p>
<ol>
<li><p>应用场景</p>
<p>Hive适合用于对一段时间内的数据进行分析查询。如 计算趋势金和网站的日志。Hive需要很长的时间才可以返回结果，因此不适合进行实时查询。</p>
<p>HBase 非常适合大数据的实时查询。Facebook使用HBase进行消息的实时分析，统计Facebook的连接数</p>
</li>
<li><p>总结</p>
<p><strong>Hive</strong> 是一种类SQL的引擎，运行MR任务。<strong>用来离线统计查询</strong>。</p>
<p><strong>HBase</strong>是一种在Hadoop之上的<strong>NoSQL的key&#x2F;value 数据库</strong>。<strong>进行实时查询</strong>。</p>
</li>
</ol>
</li>
<li><p>HBase角色</p>
<p>HMaster 和 RegionServer</p>
<ol>
<li><p>HMaster</p>
<ol>
<li><p>监控RegionServer</p>
</li>
<li><p>处理RegionServer故障转移</p>
</li>
<li><p>处理元数据的变更</p>
</li>
<li><p>处理region的分配或移除</p>
</li>
<li><p>在空闲时间进行数据的负载均衡</p>
</li>
<li><p>通过Zookeeper发布自己的位置给客户端</p>
</li>
</ol>
</li>
<li><p>RegionServer</p>
<ol>
<li><p>负责存储HBase的实际数据</p>
</li>
<li><p>处理分配给它的Region</p>
</li>
<li><p>刷新缓存到HDFS</p>
</li>
<li><p>维护HLog</p>
</li>
<li><p>执行压缩</p>
</li>
<li><p>负责处理Region分片</p>
</li>
</ol>
</li>
<li><p>组件</p>
<ul>
<li><p>Write-Ahead logs</p>
<p>HBase读写数据的时候，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p>
</li>
<li><p>HFile</p>
<p>原始数据的实际存储文件。</p>
</li>
<li><p>Store</p>
<p>HFile存储在Store中，一个Store对应HBase表中的一个列族。</p>
</li>
<li><p>MemStore</p>
<p>内存存储，位于内存中，用来保存当前的数据操作。</p>
<p>当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。</p>
</li>
<li><p>Region</p>
<p>Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。（我的理解类似于分区）</p>
</li>
</ul>
</li>
<li><p>HBase的启动</p>
<ul>
<li>bin&#x2F;start-hbase.sh</li>
<li>bin&#x2F;stop-hbase.sh</li>
</ul>
<p>如果使用的是JDK8以上版本，则应在hbase-evn.sh中</p>
<p>移除“HBASE_MASTER_OPTS”和“HBASE_REGIONSERVER_OPTS”配置。</p>
<ul>
<li><p>查看Hbase页面</p>
<p><a target="_blank" rel="noopener" href="http://masterhost:16010/">http://masterhost:16010</a></p>
</li>
</ul>
</li>
</ol>
<p><strong>简单说下HBase的表分区和索引管理</strong></p>
<ul>
<li>将Table 中的数据根据rowKey 字段划分为多个HRegion</li>
<li>HRegion分配给RegionServer管理</li>
</ul>
</li>
</ul>
<h2 id="2-HBase的使用"><a href="#2-HBase的使用" class="headerlink" title="2. HBase的使用"></a>2. HBase的使用</h2><h3 id="2-1-简单实用"><a href="#2-1-简单实用" class="headerlink" title="2.1 简单实用"></a>2.1 简单实用</h3><h4 id="2-1-1-基本操作"><a href="#2-1-1-基本操作" class="headerlink" title="2.1.1 基本操作"></a>2.1.1 基本操作</h4><ul>
<li><p>进入客户端</p>
<p>bin&#x2F;hbase shell</p>
</li>
<li><p>帮助命令：help</p>
</li>
<li><p>查看当前数据库的表：list</p>
</li>
</ul>
<h4 id="2-2-2-表的操作"><a href="#2-2-2-表的操作" class="headerlink" title="2.2.2 表的操作"></a>2.2.2 表的操作</h4><p>涉及的名称：表名，列族，rowkey，列名，值</p>
<p>ps：由于是key value结构的数据，可以<strong>把列族理解为一个对象</strong>，<strong>列理解为对象的一个属性</strong></p>
<ul>
<li><p>创建表 student – <strong>student 表名，info 列族</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &#x27;student&#x27;,&#x27;info&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>插入数据列表 – <strong>name,sex,age 列名，</strong> <strong>1001&#x2F;1002 rowkey</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;,&#x27;Thomas&#x27;</span><br><span class="line">put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:sex&#x27;,&#x27;male&#x27;</span><br><span class="line">put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;18&#x27;</span><br><span class="line">put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:name&#x27;,&#x27;Janna&#x27;</span><br><span class="line">put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;,&#x27;female&#x27;</span><br><span class="line">put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:age&#x27;,&#x27;20&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>扫描查看表数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scan &#x27;student&#x27;</span><br><span class="line"></span><br><span class="line">scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;, STOPROW  =&gt; &#x27;1001&#x27;&#125;</span><br><span class="line"></span><br><span class="line">scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看表结构</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe ‘student’</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新指定字段的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;,&#x27;Nick&#x27;</span><br><span class="line"></span><br><span class="line">put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;100&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看“指定行”或“指定列族:列”的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">get &#x27;student&#x27;,&#x27;1001&#x27; //查看指定行</span><br><span class="line"></span><br><span class="line">get &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27; //指定列族info 指定列name</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除数据</p>
<p>删除某 rowkey 的全部数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deleteall &#x27;student&#x27;,&#x27;1001&#x27;</span><br></pre></td></tr></table></figure>

<p>删除某 rowkey 的某一列数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>清空表数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">disable &#x27;student&#x27;</span><br><span class="line"></span><br><span class="line">truncate &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<p>PS: 清空表的操作顺序为先disable，然后再truncating。</p>
</li>
<li><p>删除表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">disable &#x27;student&#x27;</span><br><span class="line"></span><br><span class="line">drop &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<p>PS: 删除表的操作顺序为先disable，然后再drop。</p>
</li>
<li><p>统计表数据的行数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count &#x27;student&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>变更表信息</p>
<p>将列族中的数据存放3个版本：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter &#x27;student&#x27;,&#123;NAME=&gt;&#x27;info&#x27;,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-2-读写流程"><a href="#2-2-读写流程" class="headerlink" title="2.2 读写流程"></a>2.2 读写流程</h3><h4 id="2-2-1-Hbase读流程"><a href="#2-2-1-Hbase读流程" class="headerlink" title="2.2.1 Hbase读流程"></a>2.2.1 Hbase读流程</h4><ol>
<li><p><strong>确定meta表所在的HRegionServer</strong>。</p>
<p>HRegionServer保存着meta表以及表数据。因此要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取meta表所在的位置信息，即找到这个meta表在哪个HRegionServer上保存着。</p>
</li>
<li><p><strong>访问meta表所在的HRegionServer,读取meta表中存放的元数据</strong>。</p>
<p>Client通过刚才获取到的HRegionServer的IP来访问Meta表所在的HRegionServer，从而读取到Meta，进而获取到Meta表中存放的元数据。</p>
</li>
<li><p><strong>根据元数据扫描Memstore和Storefile 查询数据</strong>。</p>
<p>Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据。</p>
</li>
<li><p>HRegionServer把查询到的数据响应给Client。</p>
</li>
</ol>
<h4 id="2-2-2-HBase写数据流程"><a href="#2-2-2-HBase写数据流程" class="headerlink" title="2.2.2 HBase写数据流程"></a>2.2.2 HBase写数据流程</h4><ol>
<li><p><strong>获取Meta表信息</strong></p>
<p>Client也是先访问zookeeper，找到Meta表，并获取Meta表信息。</p>
</li>
<li><p><strong>确定RegionServer服务器和Region。</strong></p>
<p>确定当前将要写入的数据所对应的RegionServer服务器和Region。</p>
</li>
<li><p><strong>发起写入数据请求</strong></p>
<p>Client向该RegionServer服务器发起写入数据请求，然后RegionServer收到请求并响应。</p>
</li>
<li><p><strong>先写HLog</strong></p>
<p>Client先把数据写入到HLog，以防止数据丢失。</p>
</li>
<li><p><strong>再写Memstore</strong></p>
</li>
<li><p>如果<strong>Hlog</strong>和<strong>Memstore均写入成功</strong>，则这条数据<strong>写入成功</strong>。</p>
<p>在此过程中，如果Memstore达到阈值，会把Memstore中的数据flush到StoreFile中。</p>
</li>
<li><p><strong>Compact合并操作 和 Split操作</strong></p>
<p>当<strong>Storefile越来越多</strong>，会触发Compact<strong>合并操作</strong>，把过多的Storefile合并成一个大的Storefile。当<strong>Storefile越来越大</strong>，Region也会越来越大，<strong>达到阈值后</strong>，会触发<strong>Split操作</strong>，<strong>将Region一分为二</strong>。</p>
</li>
</ol>
<p><strong>PS: 因为内存空间的限制，溢写文件必定伴随着大量小文件的产生。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">总结：写入的时候 Hlog 和 Memstore均写入。</span><br><span class="line"></span><br><span class="line">1. Memstore 达到阈值，flush 到 Storefile 中。</span><br><span class="line">2. Storefile太多，Compact合并操作，合成大的Storefile。</span><br><span class="line">3. Storefile太大，Region达到阈值后触发Split操作，将Region一分为二。</span><br></pre></td></tr></table></figure>

<h2 id="3-HBase-MapReduce。。"><a href="#3-HBase-MapReduce。。" class="headerlink" title="3. HBase - MapReduce。。"></a>3. HBase - MapReduce。。</h2><ol>
<li>使用MapReduce将数据从本地文件系统导入到HBase的表。</li>
<li>从HBase中读取一些原始数据后使用MapReduce做数据分析</li>
</ol>
<h3 id="3-1-使用官方的-MR"><a href="#3-1-使用官方的-MR" class="headerlink" title="3.1 使用官方的 MR"></a>3.1 使用官方的 MR</h3><h2 id="4-HBase-Hive。。"><a href="#4-HBase-Hive。。" class="headerlink" title="4. HBase - Hive。。"></a>4. HBase - Hive。。</h2><h2 id="5-常用-Shell-操作"><a href="#5-常用-Shell-操作" class="headerlink" title="5. 常用 Shell 操作"></a>5. 常用 Shell 操作</h2><ol>
<li><p><strong>status</strong></p>
<p>例如显示服务器状态：status ‘linux01’</p>
</li>
<li><p><strong>whoami</strong></p>
<p>显示HBase 当前用户：hbase&gt; whoami</p>
</li>
<li><p><strong>list</strong></p>
<p>显示当前所有的表：hbase&gt; list</p>
</li>
<li><p><strong>count</strong></p>
<p>统计指定表的记录数：count ‘hbase_student’</p>
</li>
<li><p><strong>describe</strong></p>
<p>展示表结构：describe ‘hbase_student’</p>
</li>
<li><p><strong>exist</strong></p>
<p>检查表是否存在：exist ‘hbase_student’</p>
</li>
<li><p><strong>is_enabled &#x2F; is_disabled</strong></p>
<p>检查表是否启用或禁用:</p>
<p>is_enabled ‘hbase_student’</p>
<p>is_disabled ‘hbase_student’</p>
</li>
<li><p><strong>alter</strong></p>
<p><strong>改变表和列族的模式</strong>：</p>
<p>为当前表增加列族：alter ‘hbase_student’, NAME &#x3D;&gt; ‘info2’, VERSIONS &#x3D;&gt; 2</p>
<p>为当前表删除列族：alter ‘hbase_student’, ‘delete’ &#x3D;&gt; ‘info2’</p>
</li>
<li><p><strong>disabled</strong></p>
<p>禁用一张表：disable ‘hbase_student’</p>
</li>
<li><p><strong>drop</strong></p>
<p>删除一张表：disable ‘hbase_student’；drop ‘hbase_student’；</p>
</li>
<li><p><strong>delete</strong></p>
<p>删除一行中一个单元格的值：delete ‘hbase_student’, ‘1001’, ‘info:name’</p>
<p> 表名 rowkey 列族:列名</p>
</li>
<li><p><strong>truncate</strong></p>
<p>清空表数据：disable ‘hbase_student’；truncate ‘hbase_student’；</p>
</li>
<li><p><strong>create</strong></p>
<p>创建表： create ‘table’,‘info’</p>
<p> 表名，列族名</p>
<p>创建多个族：</p>
<p>create ‘table’,{NAME &#x3D;&gt; ‘info1’}, {NAME &#x3D;&gt; ‘info2’}, {NAME &#x3D;&gt; ‘info3’}</p>
</li>
</ol>
<h2 id="6-数据的备份与恢复"><a href="#6-数据的备份与恢复" class="headerlink" title="6. 数据的备份与恢复"></a>6. 数据的备份与恢复</h2><h3 id="6-1-备份"><a href="#6-1-备份" class="headerlink" title="6.1 备份"></a>6.1 备份</h3><p>停止HBase服务后，使用distcp命令运行MapReduce任务进行备份，将数据备份到另一个地方，可以是同一个集群，也可以是专用的备份集群。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/hadoop distcp \</span><br><span class="line">hdfs://masterhost:8020/hbase \</span><br><span class="line">hdfs://masterhost:8020/HbaseBackup/backup20191016</span><br></pre></td></tr></table></figure>

<p><strong>PS: 执行该操作，一定要开启Yarn服务</strong></p>
<h3 id="6-2-恢复"><a href="#6-2-恢复" class="headerlink" title="6.2 恢复"></a>6.2 恢复</h3><p>跟备份方法一样，将数据整个移动回来即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/hadoop distcp \</span><br><span class="line">hdfs://masterhost:8020/HbaseBackup/backup20171016 \</span><br><span class="line">hdfs://masterhost:8020/hbase</span><br></pre></td></tr></table></figure>

<h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><p>不一定所有的企业都会使用HBase，大数据的框架可以是相互配合相互依赖的，同时，根据不同的业务，部分框架之间的使用也可以是相互独立的。</p>
<p>例如有些企业在处理整个业务时，只是用HDFS+Spark部分的内容。</p>
<p>一定要有宏观思维，了解其框架特性，不一定非要在所有的业务中使用所有的框架，要具体情况具体分析，酌情选择。</p>
<h2 id="8-协处理器"><a href="#8-协处理器" class="headerlink" title="8. 协处理器"></a>8. 协处理器</h2><h3 id="8-1-简介"><a href="#8-1-简介" class="headerlink" title="8.1 简介"></a>8.1 简介</h3><h4 id="8-1-1-起源"><a href="#8-1-1-起源" class="headerlink" title="8.1.1 起源"></a>8.1.1 起源</h4><p>Hbase 作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执 行求和、计数、排序等操作。</p>
<p>比如，在旧版本的(&lt;0.92)Hbase 中，统计数据表的总行数，需 要使用 Counter 方法，执行一次 MapReduce Job 才能得到。</p>
<p><strong>如果直接将计算过程放置在 server 端，能够减少通讯开销，从而获 得很好的性能提升。</strong></p>
<p>于是，HBase 在 0.92 之后引入了协处理器(coprocessors)，能够轻易<strong>建立二次索引</strong>、<strong>复杂过滤器(谓词下推)以及访问控制</strong>等。</p>
<h4 id="8-1-2-介绍"><a href="#8-1-2-介绍" class="headerlink" title="8.1.2 介绍"></a>8.1.2 介绍</h4><p>协处理器有两种：<strong>observer 和 endpoint</strong></p>
<p>　　<strong>Observer</strong> <strong>类似于传统数据库中的触发器</strong>，当发生某些事件的时候这类协处理器会被 Server 端调用。<strong>Observer Coprocessor</strong> 就是一些散布在 HBase Server 端代码中的 hook 钩子， 在固定的事件发生时被调用。</p>
<p>比如：put 操作之前有钩子函数 prePut，该函数在 put 操作执 行前会被 Region Server 调用；在 put 操作之后则有 postPut 钩子函数。</p>
<p>以 HBase0.92 版本为例，它提供了三种观察者接口：</p>
<p> <strong>RegionObserver</strong>：提供客户端的数据操纵事件钩子：<strong>Get</strong>、<strong>Put</strong>、<strong>Delete</strong>、<strong>Scan</strong> 等。</p>
<p> <strong>WALObserver</strong>：提供 WAL 相关操作钩子。</p>
<p> <strong>MasterObserver</strong>：提供 DDL-类型的操作钩子。如创建、删除、修改数据表等。</p>
<p> 到 0.96 版本又新增一个 <strong>RegionServerObserver</strong></p>
<p><strong>补充：WAL的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。</strong></p>
<p> <strong>Endpoint</strong> 协处理器类似传统数据库中的存储过程，客户端可以调用这些 Endpoint 协处 理器执行一段 Server 端代码，并将 Server 端代码的结果返回给客户端进一步处理，<strong>最常见 的用法就是进行聚集操作</strong>。</p>
<p> 如果没有协处理器，当用户需要找出一张表中的最大数据，即 max 聚合操作，就必须进行全表扫描，在客户端代码内遍历扫描结果，并执行求最大值的 操作。这样的方法无法利用底层集群的并发能力，而将所有计算都集中到 Client 端统一执行， 势必效率低下。</p>
<p> 利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase 将利用底层 cluster 的多个节点<strong>并发执行求最大值的操作</strong>。即在每个 Region 范围内执行求最大值的代码，将<strong>每个 Region 的最大值在 Region Server 端计算出</strong>，仅仅将该 max 值返回给客 户端。在<strong>客户端进一步将多个 Region 的最大值进一步处理而找到其中的最大值</strong>。</p>
<h4 id="8-1-3-协处理器原理"><a href="#8-1-3-协处理器原理" class="headerlink" title="8.1.3 协处理器原理"></a>8.1.3 协处理器原理</h4><p>以<em>observer</em> put请求为例：</p>
<p> 1、客户端发出 put 请求</p>
<p>　　2、该请求被分派给合适的 RegionServer 和 region</p>
<p>　　3、coprocessorHost 拦截该请求，然后在该表上登记的每个 RegionObserver 上调用 prePut()</p>
<p>　　4、如果没有被 prePut()拦截，该请求继续送到 region，然后进行处理</p>
<p>　　5、region 产生的结果再次被 CoprocessorHost 拦截，调用 postPut()</p>
<p>　　6、假如没有 postPut()拦截该响应，最终结果被返回给客户端</p>
<h4 id="8-1-4-总结"><a href="#8-1-4-总结" class="headerlink" title="8.1.4 总结"></a>8.1.4 总结</h4><p> <strong>Observer</strong> 允许集群在正常的客户端操作过程中可以有不同的行为表现</p>
<p>　　<strong>Endpoint</strong> 允许<strong>扩展集群的能力</strong>，对客户端应用开放新的运算命令</p>
<p>　　<strong>Observer</strong> 类似于 RDBMS 中的<strong>触发器</strong>，主要在服务端工作</p>
<p>　　<strong>Endpoint</strong> 类似于 RDBMS 中的<strong>存储过程</strong>，主要在服务端工作</p>
<p>　　</p>
<p> Observer 可以实现权限管理、优先级设置、监控、ddl 控制、<strong>二级索引</strong>等功能</p>
<p>　　Endpoint 可以实现 <strong>min、max、avg、sum、count、distinct、group by</strong> 等功能</p>
<h4 id="8-1-5-WAL"><a href="#8-1-5-WAL" class="headerlink" title="8.1.5 WAL"></a>8.1.5 WAL</h4><p><strong>WAL(Write-Ahead-Log)<strong>预写日志是Hbase的RegionServer在处理数据插入和删除的过程中用来</strong>记录操作内容的一种日志</strong>。</p>
<p>在每次Put、Delete等一条记录时，首先将其数据写入到RegionServer对应的HLog文件中去。</p>
<p>客户端向RegionServer端提交数据的时候，会先写入WAL日志，只有当WAL日志写入成功的时候，客户端才会被告诉提交数据成功。<strong>如果写WAL失败会告知客户端提交失败，这其实就是数据落地的过程。</strong></p>
<p>在一个RegionServer上的所有Region都共享一个HLog，一次数据的提交先写入WAL，写入成功后，再写入menstore之中。</p>
<p>当menstore的值达到一定的时候，就会形成一个StoreFile。</p>
<ul>
<li><p>HBase容错处理</p>
<p>WAL记载了每一个RegionServer对应的HLog。</p>
<p>RegionServer1或者RegionServer1上某一个regiong挂掉了，都会迁移到其它的机器上处理，重新操作，进行恢复。</p>
<p>当RegionServer意外终止的时候，Master会通过Zookeeper感知到，Master首先会处理遗留下来的HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应的Region目录下，然后再将实效的Region重新分配，领取到这些Region的RegionMaster发现有历史的HLog需要处理，因此会Replay HLog的数据到Memstore之中，然后flush数据到StoreFiles，完成数据的恢复。</p>
</li>
<li><p>HBase和HDFS的关系</p>
<p>相同点：</p>
<ul>
<li>二者都具有良好的容错性和扩展性，都可以扩展成百千上万个结点</li>
</ul>
<p>不同点：</p>
<ul>
<li>HDFS<strong>适合批处理场景</strong>。</li>
<li>HDFS<strong>不支持数据的随机查找</strong>、<strong>不适合增量数据处理</strong>、<strong>不支持数据更新</strong>。</li>
</ul>
<p>关系：</p>
<ul>
<li>Hbase内存管理的所有文件都存储在HDFS之中。</li>
</ul>
</li>
</ul>
<h3 id="8-2-协处理器的加载方式"><a href="#8-2-协处理器的加载方式" class="headerlink" title="8.2 协处理器的加载方式"></a>8.2 协处理器的加载方式</h3><p>协处理器的加载方式有两种，我们称之为<strong>静态加载方式（Static Load）和动态加载方式 （Dynamic Load）</strong>。</p>
<p>静态加载的协处理器称之为 <strong>System Coprocessor</strong>，动态加载的协处理器称 之为 <strong>Table Coprocessor</strong>。</p>
<h4 id="8-2-1-静态加载"><a href="#8-2-1-静态加载" class="headerlink" title="8.2.1 静态加载"></a>8.2.1 静态加载</h4><p>通过修改hbase-site.xml 这个文件实现，启动全局的aggregation, 能够操作所有表上的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;	</span><br><span class="line">	&lt;name&gt;hbase.coprocessor.user.region.classes&lt;/name&gt;	</span><br><span class="line">	&lt;value&gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>为所有 table 加载了一个 cp class，可以用”,”分割加载多个 class。</p>
<h4 id="8-2-2-动态加载"><a href="#8-2-2-动态加载" class="headerlink" title="8.2.2 动态加载"></a>8.2.2 动态加载</h4><p>启用表 aggregation，只对特定的表生效。通过 HBase Shell 来实现。</p>
<ol>
<li><p>停用表　　disable ‘tablename’</p>
</li>
<li><p>添加协处理器　　</p>
<p>alter ‘tablename’, METHOD &#x3D;&gt; ‘table_att’, ‘coprocessor’ &#x3D;&gt; ‘hdfs:&#x2F;&#x2F;myha01&#x2F;hbase&#x2F;guanzhu.jar|com.hypers.insight.HbaseCoprocessorTest|1001|’</p>
</li>
<li><p>启用表　　enable ‘tablename’</p>
</li>
</ol>
<h4 id="8-2-3-协处理器卸载"><a href="#8-2-3-协处理器卸载" class="headerlink" title="8.2.3 协处理器卸载"></a>8.2.3 协处理器卸载</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">disable &#x27;mytable&#x27;</span><br><span class="line"></span><br><span class="line">alter &#x27;mytable&#x27;,METHOD=&gt;&#x27;table_att_unset&#x27;,NAME=&gt;&#x27;coprocessor$1&#x27;</span><br><span class="line"></span><br><span class="line">enable &#x27;mytable&#x27;</span><br></pre></td></tr></table></figure>

<h3 id="8-3-Endpoint"><a href="#8-3-Endpoint" class="headerlink" title="8.3 Endpoint"></a>8.3 Endpoint</h3><ul>
<li>与Observer类型不同的是，<strong>Endpoint协处理器需要与服务区直接通信</strong>，服务端是对于Protobuf Service的实现，所以两者直接会有一个基于protocl的RPC接口，客户端和服务端都需要进行基于接口的代码逻辑实现。</li>
<li>不同于Observer协处理器，EndPoint由于需要同region进行rpc服务的通信，以及客户端出数据的归并，需要自行实现客户端代码。</li>
<li></li>
</ul>
<h3 id="8-4-关于二级索引"><a href="#8-4-关于二级索引" class="headerlink" title="8.4 关于二级索引"></a>8.4 关于二级索引</h3><ul>
<li><p>HBase的局限性</p>
<p>HBase本身<strong>只提供基于行键和全表扫描的查询</strong>，而<strong>行键索引单一</strong>，对于多维度的查询困难。</p>
</li>
<li><p>常见的二级索引方案</p>
<p>HBase的<strong>一级索引</strong>就是<strong>rowkey</strong>，我们只能通过rowkey进行检索。如果我们相对HBase里面列族的<strong>列列进行一些组合查询</strong>，就<strong>需要采用HBase的二级索引方案</strong>来进行多条件的查询。</p>
<ul>
<li><p>MapReduce 方案</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wypersist/article/details/79830811">https://blog.csdn.net/wypersist/article/details/79830811</a></p>
</li>
<li><p>ITHBASE（Indexed-Transanctional HBase）方案</p>
</li>
<li><p>IHBASE（Index HBase）方案</p>
</li>
<li><p>Hbase Coprocessor(协处理器)方案</p>
</li>
<li><p>Solr+hbase方案</p>
</li>
<li><p>CCIndex（complementalclustering index）方案</p>
</li>
</ul>
</li>
</ul>
<p>这里主要学习Hbase Coprocessor(协处理器)方案 。</p>
<p><strong>二级索引的本质就是建立各列值与行键之间的映射关系</strong></p>
<p><a href="20220710-4.jpg"><img src="/../../../../images/20220710-4.jpg" alt="20220710-4.jpg"></a></p>
<p>如上图1，当要对F:C1这列建立索引时，只需要建立F:C1各列值到其对应行键的映射关系，如C11-&gt;RK1等（第二张表），这样就完成了对F:C1列值的二级索引的构建。</p>
<p>当要查询符合F:C1&#x3D;C11对应的F:C2的列值时（即根据C1&#x3D;C11来查询C2的值,第三张表青色部分）</p>
<p>其查询步骤如下：</p>
<ul>
<li>根据C1&#x3D;C11到索引数据中查找其对应的RK，查询得到其对应的RK&#x3D;RK1</li>
<li>得到RK1后就自然能根据RK1来查询C2的值了 这是构建二级索引大概思路，其他组合查询的联合索引的建立也类似。</li>
</ul>
<p><strong>HBase在0.92之后引入了coprocessors，提供了一系列的钩子，让我们能够轻易实现访问控制和二级索引的特性。</strong></p>
<h2 id="9-案例1（observer二级索引）"><a href="#9-案例1（observer二级索引）" class="headerlink" title="9. 案例1（observer二级索引）"></a>9. 案例1（observer二级索引）</h2><p>该例子使用RegionObserver实现在<strong>写主表之前将索引数据先写到另外一个表</strong></p>
<p>相当于表一添加，触发表二信息的添加或者更新</p>
<p>以关注表，粉丝表为例。</p>
<p>关注表：zhangsan 关注了 gulinazha</p>
<p>粉丝表：zhangsan 是gulinazha 的粉丝</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">put &#x27;insight:bonaGuanzhu&#x27;, &#x27;zhangsan&#x27;, &#x27;cf:star&#x27;, &#x27;gulinazha&#x27;</span><br><span class="line">put &#x27;insight:bonaFans&#x27;, &#x27;gulinazha&#x27;, &#x27;cf:fensi&#x27;, &#x27;zhangsan&#x27; //触发添加</span><br></pre></td></tr></table></figure>

<ol>
<li>java实现代码</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">package com.xxx.bona;</span><br><span class="line"></span><br><span class="line">import java.util.*;import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.client.Connection;</span><br><span class="line">import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.client.Table;</span><br><span class="line">import org.apache.hadoop.hbase.*;</span><br><span class="line">import org.apache.hadoop.hbase.client.Durability;</span><br><span class="line">import org.apache.hadoop.hbase.regionserver.wal.WALEdit;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.ObserverContext;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;</span><br><span class="line"></span><br><span class="line">public class HbaseCoprocessorTest extends BaseRegionObserver&#123;    </span><br><span class="line">	static Connection connection;    </span><br><span class="line">	static Configuration configuration;    </span><br><span class="line">	static Table table = null;    </span><br><span class="line">	static &#123;        </span><br><span class="line">		configuration = HBaseConfiguration.create();        </span><br><span class="line">		configuration.addResource(&quot;hbase-site.xml&quot;);        </span><br><span class="line">		try &#123;            </span><br><span class="line">			// 取得一个数据库连接对象            </span><br><span class="line">			connection = ConnectionFactory.createConnection(configuration);        </span><br><span class="line">		&#125; catch (IOException e) &#123;            </span><br><span class="line">			e.printStackTrace();        </span><br><span class="line">		&#125;    </span><br><span class="line">	&#125;    </span><br><span class="line">	</span><br><span class="line">	/***     </span><br><span class="line">	* 此方法是在put方法调用之前进行调用     </span><br><span class="line">	* @param e     </span><br><span class="line">	* @param put 是要进行插入的那条数据     </span><br><span class="line">	* @param edit     </span><br><span class="line">	* @param durability     </span><br><span class="line">	* @throws IOException     </span><br><span class="line">	*/    </span><br><span class="line">	@Override    </span><br><span class="line">	public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, </span><br><span class="line">	WALEdit edit, Durability durability) throws IOException &#123;        </span><br><span class="line">		//获取put对象里面的rowkey&#x27;zhangsan&#x27;        </span><br><span class="line">		byte[] row = put.getRow();        </span><br><span class="line">		table = connection.getTable(TableName.valueOf(&quot;test:bonaFans&quot;));        </span><br><span class="line">		//获取put对象里面的cell        </span><br><span class="line">		List&lt;Cell&gt; list = put.get(&quot;cf&quot;.getBytes(), &quot;star&quot;.getBytes());        </span><br><span class="line">		Cell cell = list.get(0);        </span><br><span class="line">		//创建一个新的put对象        </span><br><span class="line">		Put new_put = new Put(cell.getValueArray());        </span><br><span class="line">		new_put.addColumn(&quot;cf&quot;.getBytes(), &quot;fensi&quot;.getBytes(), row);        </span><br><span class="line">		table.put(new_put);        </span><br><span class="line">		connection.close();    </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>打包成jar, 重命名为 guanzhu.jar 并上传HDFS目录&#x2F;bona&#x2F;hbase下面</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put guanzhu.jar /bona/hbase</span><br></pre></td></tr></table></figure>


</li>
<li><p>打开 hbase shell</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">disable &#x27;test:bonaGuanzhu&#x27;</span><br><span class="line"></span><br><span class="line">alter &#x27;test:bonaGuanzhu&#x27;, METHOD =&gt; &#x27;table_att&#x27;, &#x27;coprocessor&#x27; =&gt; &#x27;hdfs://nameserve1/bona/hbase/guanzhu.jar|com.xxx.bona.HbaseCoprocessorTest|1001|&#x27;</span><br><span class="line"></span><br><span class="line">enable &#x27;test:bonaGuanzhu&#x27;</span><br><span class="line"></span><br><span class="line">desc &#x27;test:bonaGuanzhu&#x27;</span><br><span class="line">put &#x27;test:bonaGuanzhu&#x27;, &#x27;zhangsan&#x27;, &#x27;cf:star&#x27;, &#x27;gulinazha&#x27;</span><br><span class="line"></span><br><span class="line">scan &#x27;test:bonaGuanzhu&#x27;</span><br><span class="line">scan &#x27;test:bonaFans&#x27;</span><br><span class="line"></span><br><span class="line">卸载协处理器</span><br><span class="line">disable &#x27;test:bonaGuanzhu&#x27;</span><br><span class="line">alter &#x27;test:bonaGuanzhu&#x27;,METHOD=&gt;&#x27;table_att_unset&#x27;,NAME=&gt;&#x27;coprocessor$1&#x27;</span><br><span class="line">enable &#x27;test:bonaGuanzhu&#x27;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="10-案例2（observer-读写分离）"><a href="#10-案例2（observer-读写分离）" class="headerlink" title="10. 案例2（observer,读写分离）"></a>10. 案例2（observer,读写分离）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">假定某个表 test:bonaRW 有A和B两个列1. 当我们向A列插入数据的时候通过协处理器像B列也插入数据。2.在读取数据的时候只允许客户端读取B列数据而不能读取A列数据。换句话说A列是只写 B列是只读的。（为了简单起见，用户在读取数据的时候需要制定列名）3. A列值必须是整数，换句话说B列值也自然都是整数4.当删除操作的时候不能指定删除B列5.当删除A列的时候同时需要删除B列6.对于其他列的删除不做检查</span><br></pre></td></tr></table></figure>

<p>代码见github. <a target="_blank" rel="noopener" href="https://www.cnblogs.com/ios123/p/6370724.html">https://www.cnblogs.com/ios123/p/6370724.html</a></p>
<h2 id="11-案例3（endpoint）"><a href="#11-案例3（endpoint）" class="headerlink" title="11. 案例3（endpoint）"></a>11. 案例3（endpoint）</h2><p>准备工作参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/hp_cpp/article/details/81561310">https://blog.csdn.net/hp_cpp/article/details/81561310</a></p>
<p>实例参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/ios123/p/6379407.html">https://www.cnblogs.com/ios123/p/6379407.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">完成hbase表 test:bona_endpoint 表的count,max,min,sum,avg</span><br><span class="line">通过代码的方式实现 协处理器的添加，使用，删除主要分为5个步骤：	1.环境准备（使用Protobuf 生成序列化类）	2.Endpoint Coprocessor服务端实现	3.Endpoint Coprocessor客户端实现	4.部署以及调用</span><br></pre></td></tr></table></figure>

<h3 id="11-1-环境准备"><a href="#11-1-环境准备" class="headerlink" title="11.1 环境准备"></a>11.1 环境准备</h3><p>HBase在HMaster、RegionServer内部，创建了RpcServer实例，并可与Client三者之间实现了Rpc调用。</p>
<p>HBase0.95版本引入了Google-Protobuf作为中间数据组织方式，并在Protobuf提供的Rpc接口之上，实现了基于服务的Rpc实现。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Protobuf Buffers是一种轻便高效的结构化数据存储格式，可以用于数据序列化。适合做数据存储或RPC数据交换格式。用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。</span><br></pre></td></tr></table></figure>

<ul>
<li><p>下载Protobuf2.5.0版本的安装包。因此maven环境使用的是2.5.0，因此这里需要和项目的版本对应哦。</p>
</li>
<li><pre><code>https://github.com/protocolbuffers/protobuf/releases/tag/v2.5.0
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- win10 下解压到指定目录，这里用root表示</span><br><span class="line"></span><br><span class="line">- 准备HBase测试表，建表脚本及测试数据如下</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  这里使用代码添加 见github：https://github.com/fenghuayangyi/hbasedemo/blob/master/src/main/java/com/hypers/insight/HbaseTest.java</span><br><span class="line">  </span><br><span class="line">  createTable(&quot;test:bona_endpoint&quot;, &quot;info&quot;);</span><br><span class="line">  </span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;001&quot;,&quot;info&quot;,&quot;sales&quot;,&quot;15.5&quot;);</span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;002&quot;,&quot;info&quot;,&quot;sales&quot;,&quot;12.8&quot;);</span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;003&quot;,&quot;info&quot;,&quot;sales&quot;,&quot;13.5&quot;);</span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;004&quot;,&quot;info&quot;,&quot;sales&quot;,&quot;11.0&quot;);</span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;005&quot;,&quot;info&quot;,&quot;sales&quot;,&quot;9.5&quot;);</span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;001&quot;,&quot;info&quot;,&quot;age&quot;,&quot;27&quot;);</span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;002&quot;,&quot;info&quot;,&quot;age&quot;,&quot;28&quot;);</span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;003&quot;,&quot;info&quot;,&quot;age&quot;,&quot;26&quot;);</span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;004&quot;,&quot;info&quot;,&quot;age&quot;,&quot;33&quot;);</span><br><span class="line">  addRowData(&quot;test:bona_endpoint&quot;,&quot;005&quot;,&quot;info&quot;,&quot;age&quot;,&quot;36&quot;);</span><br><span class="line">  </span><br><span class="line">  也可以通过 hbase shell:</span><br><span class="line">  create &#x27;test:bona_endpoint&#x27;</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;001&#x27;,&#x27;info:sales&#x27;,15.5</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;002&#x27;,&#x27;info:sales&#x27;,12.8</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;003&#x27;,&#x27;info:sales&#x27;,13.5</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;004&#x27;,&#x27;info:sales&#x27;,11.0</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;005&#x27;,&#x27;info:sales&#x27;,9.5</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;001&#x27;,&#x27;info:age&#x27;,27</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;002&#x27;,&#x27;info:age&#x27;,28</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;003&#x27;,&#x27;info:age&#x27;,26</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;004&#x27;,&#x27;info:age&#x27;,33</span><br><span class="line">  put &#x27;test:bona_endpoint&#x27;,&#x27;005&#x27;,&#x27;info:age&#x27;,36</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>使用Protobuf生成序列化类</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.准备MyFirstCoprocessor.proto文件,放到root/bin目录下 </span><br><span class="line">见githubhttps://github.com/fenghuayangyi/hbasedemo/blob/master/src/filebackup/MyFirstCoprocessor.proto</span><br><span class="line"></span><br><span class="line">2.进入cmd命令行,切换到root/bin目录，执行如下命令生成Java类</span><br><span class="line">protoc --java_out=./ MyFirstCoprocessor.proto</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="11-2-Endpoint-Coprocessor服务端实现"><a href="#11-2-Endpoint-Coprocessor服务端实现" class="headerlink" title="11.2 Endpoint Coprocessor服务端实现"></a>11.2 Endpoint Coprocessor服务端实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1.maven中添加依赖 见github主要涉及：</span><br><span class="line">hadoop-client,</span><br><span class="line">hadoop-common,</span><br><span class="line">hbase-client,</span><br><span class="line">hbase-examples,</span><br><span class="line">protobuf-java</span><br><span class="line"></span><br><span class="line">2.将root/bin目录下生成的类拷贝到指定的package目录下。</span><br><span class="line">与.proto文件指定的java_package包目录一致。</span><br><span class="line"></span><br><span class="line">3.新建server包，在包下新建MyFirstCoprocessorEndpoint实现类 内容见githubhttps://github.com/fenghuayangyi/hbasedemo/blob/master/src/main/java/com/hypers/insight/hbaseendpoint/server/MyFirstCoprocessorEndPoint.java</span><br></pre></td></tr></table></figure>

<h3 id="11-3-Endpoint-Coprocessor客户端实现"><a href="#11-3-Endpoint-Coprocessor客户端实现" class="headerlink" title="11.3 Endpoint Coprocessor客户端实现"></a>11.3 Endpoint Coprocessor客户端实现</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">新建client包，在包下新建MyFirstCoprocessExample.java 内容见githubhttps://github.com/fenghuayangyi/hbasedemo/blob/master/src/main/java/com/hypers/insight/hbaseendpoint/client/MyFirstCoprocessExample.java</span><br></pre></td></tr></table></figure>

<h3 id="11-4-部署以及调用"><a href="#11-4-部署以及调用" class="headerlink" title="11.4 部署以及调用"></a>11.4 部署以及调用</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.maven 编译代码</span><br><span class="line"></span><br><span class="line">2.将编译好的代码上传到hdfs指定目录    先将jar通过FZ传到hadoop的master node,然后将其传到hdfs指定目录 </span><br><span class="line">    hadoop fs -mkdir -p /bona/hbase    </span><br><span class="line">	hadoop fs -put /.../bona/hbasedemo-1.0-SNAPSHOT.jar /bona/hbase/hbasedemo-1.0-SNAPSHOT.jar    </span><br><span class="line">	hadoop fs -ls /bona/hbase</span><br><span class="line">	</span><br><span class="line">3.运行MyFirstCoprocessorExample代码，查看运行结果。校验结果的正确性。</span><br></pre></td></tr></table></figure>

<h3 id="11-5-总结"><a href="#11-5-总结" class="headerlink" title="11.5 总结"></a>11.5 总结</h3><p>开发HBase的Endpoint Coprocessor借助于Protobuf生成RPC请求数据交互类，我们只需要在生成的类基础上实现业务即可。</p>
<p>HBase自带的也有AggregateImplementation类实现列的聚合，原生的不能同时对多个列进行聚合处理，如果需要多次聚合则需要多次调用RPC请求，HBase数据在<strong>不断的写入会出现每次聚合的结果有偏差</strong>，本示例<strong>将聚合放在一个RPC中处理可以减少RPC的请求次数并确保查询条件相同的情况下不会出现数据不一致问题</strong>。</p>

    </div>

    <div></div>
  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#HBase"><span class="toc-text">HBase</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-HBase-%E4%BB%8B%E7%BB%8D"><span class="toc-text">1. HBase 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-HBase%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">2. HBase的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E7%AE%80%E5%8D%95%E5%AE%9E%E7%94%A8"><span class="toc-text">2.1 简单实用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-1-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-text">2.1.1 基本操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-%E8%A1%A8%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="toc-text">2.2.2 表的操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B"><span class="toc-text">2.2 读写流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-Hbase%E8%AF%BB%E6%B5%81%E7%A8%8B"><span class="toc-text">2.2.1 Hbase读流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-HBase%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-text">2.2.2 HBase写数据流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-HBase-MapReduce%E3%80%82%E3%80%82"><span class="toc-text">3. HBase - MapReduce。。</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%BD%BF%E7%94%A8%E5%AE%98%E6%96%B9%E7%9A%84-MR"><span class="toc-text">3.1 使用官方的 MR</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-HBase-Hive%E3%80%82%E3%80%82"><span class="toc-text">4. HBase - Hive。。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%B8%B8%E7%94%A8-Shell-%E6%93%8D%E4%BD%9C"><span class="toc-text">5. 常用 Shell 操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%95%B0%E6%8D%AE%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D"><span class="toc-text">6. 数据的备份与恢复</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%A4%87%E4%BB%BD"><span class="toc-text">6.1 备份</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%81%A2%E5%A4%8D"><span class="toc-text">6.2 恢复</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%80%BB%E7%BB%93"><span class="toc-text">7. 总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-text">8. 协处理器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E7%AE%80%E4%BB%8B"><span class="toc-text">8.1 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-1-%E8%B5%B7%E6%BA%90"><span class="toc-text">8.1.1 起源</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-2-%E4%BB%8B%E7%BB%8D"><span class="toc-text">8.1.2 介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-3-%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E5%8E%9F%E7%90%86"><span class="toc-text">8.1.3 协处理器原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-4-%E6%80%BB%E7%BB%93"><span class="toc-text">8.1.4 总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-5-WAL"><span class="toc-text">8.1.5 WAL</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E7%9A%84%E5%8A%A0%E8%BD%BD%E6%96%B9%E5%BC%8F"><span class="toc-text">8.2 协处理器的加载方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8-2-1-%E9%9D%99%E6%80%81%E5%8A%A0%E8%BD%BD"><span class="toc-text">8.2.1 静态加载</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-2-2-%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD"><span class="toc-text">8.2.2 动态加载</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-2-3-%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E5%8D%B8%E8%BD%BD"><span class="toc-text">8.2.3 协处理器卸载</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-Endpoint"><span class="toc-text">8.3 Endpoint</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-%E5%85%B3%E4%BA%8E%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95"><span class="toc-text">8.4 关于二级索引</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%A1%88%E4%BE%8B1%EF%BC%88observer%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%EF%BC%89"><span class="toc-text">9. 案例1（observer二级索引）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E6%A1%88%E4%BE%8B2%EF%BC%88observer-%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%EF%BC%89"><span class="toc-text">10. 案例2（observer,读写分离）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E6%A1%88%E4%BE%8B3%EF%BC%88endpoint%EF%BC%89"><span class="toc-text">11. 案例3（endpoint）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-text">11.1 环境准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-Endpoint-Coprocessor%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%AE%9E%E7%8E%B0"><span class="toc-text">11.2 Endpoint Coprocessor服务端实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-Endpoint-Coprocessor%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B0"><span class="toc-text">11.3 Endpoint Coprocessor客户端实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-4-%E9%83%A8%E7%BD%B2%E4%BB%A5%E5%8F%8A%E8%B0%83%E7%94%A8"><span class="toc-text">11.4 部署以及调用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-5-%E6%80%BB%E7%BB%93"><span class="toc-text">11.5 总结</span></a></li></ol></li></ol></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by/4.0/">知识共享署名 4.0 国际许可协议</a>
    <span>进行许可。 转载时请注明原文链接。</span>
</div>
<div class="share" style="width: 100%;">
  <img src="/../../../../images/wechat.png" alt="Running Geek" style="margin: auto; display: block;"/>

  <div style="margin: auto; text-align: center; font-size: 0.8em; color: grey;">微信扫一扫向我投食</div>
  
</div>

  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/2019/10/16/Scala%E7%AC%94%E8%AE%B0/" rel="next" title="Scala笔记">
          Scala笔记
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/2019/11/18/HBase/HBase%E7%AC%94%E8%AE%B0/" rel="prev" title="HBase笔记">
            HBase笔记
          </a>
          <span>〉</span>
        
      </div>
    </div>
  


    </div>

    

  </div>
  <footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" target="_blank" rel="noopener" href="https://zshlovely.github.io/">首页</a> |
        <a class="bottom-item" href="https://zshlovely.github.io/" target="_blank">主站</a> |
        <a class="bottom-item" href="https://github.com/fenghuayangyi" target="_blank">GitHub</a>
    </div>
</footer>
  

<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>

  



</body>
</html>
